{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "price_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyphamhung10/khoa_luan/blob/master/price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "mlCxjIkEb8Gv",
        "colab_type": "code",
        "outputId": "8e2da332-b5da-4d24-f354-4110940d31d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"working.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1llsccnklCzs7EZSELP6MVysncixaZnQR\n",
        "\n",
        "## Notebook settings\n",
        "\"\"\"\n",
        "# region Import\n",
        "# Data download\n",
        "# Import basic\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "# Init google drive\n",
        "# from google.colab import drive\n",
        "from datetime import datetime\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Plottool\n",
        "import plotly.graph_objs as go\n",
        "# IPython\n",
        "from IPython.display import display\n",
        "# Hyperopt bayesian optimization\n",
        "from hyperopt import hp, Trials, tpe, fmin, STATUS_OK, partial\n",
        "# Keras\n",
        "from keras import Sequential\n",
        "from keras import optimizers\n",
        "from keras.activations import softmax\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint  \n",
        "from keras.initializers import random_normal, Ones \n",
        "from keras.layers import LSTM, Dropout, Input\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "# SKLearn\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "TgJgqIMKb8G9",
        "colab_type": "code",
        "outputId": "b60e540b-1b9a-42ca-fa47-e9c9ae7f23bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "current_timestamp = datetime.now().strftime('%d%m%Y_%H%M%S')\n",
        "if not IN_COLAB:\n",
        "    \n",
        "    # region File mount and config\n",
        "    # drive.mount('/content/gdrive', force_remount=True)\n",
        "    root_dir = \"\"\n",
        "    \n",
        "    time_dir = os.path.join(root_dir, \"result\")\n",
        "    time_dir = os.path.join(time_dir, current_timestamp)\n",
        "    \n",
        "    data_dir = root_dir + 'data'\n",
        "    model_dir = os.path.join(time_dir, 'model')\n",
        "    plot_dir = os.path.join(time_dir, 'plot')\n",
        "    result_dir = os.path.join(time_dir, 'result')\n",
        "    # Create folder if not exists\n",
        "    \n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "        \n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    \n",
        "    if not os.path.exists(plot_dir):\n",
        "        os.makedirs(plot_dir)\n",
        "        \n",
        "    if not os.path.exists(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "else:\n",
        "    # region File mount and config\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    root_dir = \"/content/gdrive/My Drive/stock\"\n",
        "    \n",
        "    time_dir = os.path.join(root_dir, \"result\")\n",
        "    \n",
        "    data_dir = os.path.join(root_dir, \"data\")\n",
        "    model_dir = os.path.join(time_dir, 'model')\n",
        "    plot_dir = os.path.join(time_dir, 'plot')\n",
        "    result_dir = os.path.join(time_dir, 'result')\n",
        "    \n",
        "pd.options.display.max_columns = 12\n",
        "pd.options.display.max_rows = 24\n",
        "\n",
        "# disable warnings in Anaconda\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "Vz_7V4YIb8HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Data Loading\n",
        "stock_name = '000001.SS'  # SSE Composite Index\n",
        "# df_org = yf.download(stock_name, start=\"1991-01-01\", end=\"2016-12-31\", interval=\"1wk\")\n",
        "df_org = pd.read_csv(f'{data_dir}/{stock_name}.csv', parse_dates=['Date'])\n",
        "df_org = df_org.sort_values('Date')\n",
        "# df_org.to_csv(f'{base_dir}/{stock_name}.csv')\n",
        "df_org.reset_index(inplace=True)\n",
        "df_org = df_org[['Date', 'Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']]\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "DB8zk98xb8HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Data ploting\n",
        "def plot_ohlc(df):\n",
        "    trace = go.Ohlc(x=df['Date'],\n",
        "                    open=df['Open'],\n",
        "                    high=df['High'],\n",
        "                    low=df['Low'],\n",
        "                    close=df['Close'],\n",
        "                    increasing=dict(line=dict(color='#58FA58')),\n",
        "                    decreasing=dict(line=dict(color='#FA5858')))\n",
        "\n",
        "    layout = {\n",
        "        'title': f'{stock_name} Historical Price',\n",
        "        'xaxis': {'title': 'Date',\n",
        "                  'rangeslider': {'visible': False}},\n",
        "        'yaxis': {'title': f'Price'}\n",
        "    }\n",
        "\n",
        "    data = [trace]\n",
        "\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    fig.write_html(os.path.join(plot_dir, '%s_ohlc.html' % (stock_name)), auto_open=False)\n",
        "\n",
        "\n",
        "plot_ohlc(df_org)\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "LfCTIEcwb8HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Create csv result file\n",
        "# File to save first results\n",
        "result_save_fname = os.path.join(result_dir, 'result_%s-%s.csv' % (stock_name, current_timestamp))\n",
        "of_connection = open(result_save_fname, 'w')\n",
        "writer = csv.writer(of_connection)\n",
        "# Write the headers to the file\n",
        "writer.writerow(['stock_name', 'year', 'loss', 'params', 'iteration', 'windows_size', 'train_time'])\n",
        "of_connection.close()\n",
        "\n",
        "# Create file to save bayer best\n",
        "bayer_save_fname = os.path.join(result_dir, 'bayer_best_%s-%s.csv' % (stock_name, current_timestamp))\n",
        "of_connection = open(bayer_save_fname, 'w')\n",
        "writer = csv.writer(of_connection)\n",
        "# Write the headers to the file\n",
        "writer.writerow(['stock_name', 'year', 'params', 'model_save_location'])\n",
        "of_connection.close()\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "3BXIGePCb8Hf",
        "colab_type": "code",
        "outputId": "c15f853d-e44c-4fc8-9876-1ea2a27b7012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# region Sample data\n",
        "\n",
        "df_org.sample(10)\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>2005-09-20</td>\n",
              "      <td>1155.041992</td>\n",
              "      <td>1222.255005</td>\n",
              "      <td>1223.564941</td>\n",
              "      <td>1147.193970</td>\n",
              "      <td>1155.041992</td>\n",
              "      <td>126200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>2006-02-28</td>\n",
              "      <td>1288.954956</td>\n",
              "      <td>1294.807007</td>\n",
              "      <td>1308.196045</td>\n",
              "      <td>1275.624023</td>\n",
              "      <td>1288.954956</td>\n",
              "      <td>121000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>1996-03-12</td>\n",
              "      <td>567.869995</td>\n",
              "      <td>563.880005</td>\n",
              "      <td>567.869995</td>\n",
              "      <td>562.299988</td>\n",
              "      <td>567.869995</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1991-04-02</td>\n",
              "      <td>121.089996</td>\n",
              "      <td>121.209999</td>\n",
              "      <td>121.720001</td>\n",
              "      <td>121.089996</td>\n",
              "      <td>121.089996</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>1998-08-04</td>\n",
              "      <td>1234.581055</td>\n",
              "      <td>1299.729004</td>\n",
              "      <td>1312.604004</td>\n",
              "      <td>1227.473022</td>\n",
              "      <td>1234.581055</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>1994-10-18</td>\n",
              "      <td>785.849976</td>\n",
              "      <td>624.520020</td>\n",
              "      <td>785.849976</td>\n",
              "      <td>624.520020</td>\n",
              "      <td>785.849976</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>2008-09-16</td>\n",
              "      <td>2236.409912</td>\n",
              "      <td>2049.812988</td>\n",
              "      <td>2269.732910</td>\n",
              "      <td>1802.331055</td>\n",
              "      <td>2236.409912</td>\n",
              "      <td>356800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>2011-02-22</td>\n",
              "      <td>2905.052979</td>\n",
              "      <td>2941.318115</td>\n",
              "      <td>2944.413086</td>\n",
              "      <td>2839.195068</td>\n",
              "      <td>2905.052979</td>\n",
              "      <td>592600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>2003-11-04</td>\n",
              "      <td>1340.097046</td>\n",
              "      <td>1363.957031</td>\n",
              "      <td>1412.910034</td>\n",
              "      <td>1322.047974</td>\n",
              "      <td>1340.097046</td>\n",
              "      <td>77800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>2010-10-26</td>\n",
              "      <td>3054.020996</td>\n",
              "      <td>3065.843994</td>\n",
              "      <td>3073.384033</td>\n",
              "      <td>2955.479980</td>\n",
              "      <td>3054.020996</td>\n",
              "      <td>852000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date        Close         Open         High          Low  \\\n",
              "768  2005-09-20  1155.041992  1222.255005  1223.564941  1147.193970   \n",
              "791  2006-02-28  1288.954956  1294.807007  1308.196045  1275.624023   \n",
              "271  1996-03-12   567.869995   563.880005   567.869995   562.299988   \n",
              "13   1991-04-02   121.089996   121.209999   121.720001   121.089996   \n",
              "396  1998-08-04  1234.581055  1299.729004  1312.604004  1227.473022   \n",
              "198  1994-10-18   785.849976   624.520020   785.849976   624.520020   \n",
              "923  2008-09-16  2236.409912  2049.812988  2269.732910  1802.331055   \n",
              "1050 2011-02-22  2905.052979  2941.318115  2944.413086  2839.195068   \n",
              "670  2003-11-04  1340.097046  1363.957031  1412.910034  1322.047974   \n",
              "1033 2010-10-26  3054.020996  3065.843994  3073.384033  2955.479980   \n",
              "\n",
              "        Adj Close  Volume  \n",
              "768   1155.041992  126200  \n",
              "791   1288.954956  121000  \n",
              "271    567.869995       0  \n",
              "13     121.089996       0  \n",
              "396   1234.581055       0  \n",
              "198    785.849976       0  \n",
              "923   2236.409912  356800  \n",
              "1050  2905.052979  592600  \n",
              "670   1340.097046   77800  \n",
              "1033  3054.020996  852000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "QTdlFgoob8Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Const\n",
        "# Declare const\n",
        "input_col = ['Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
        "output_col = ['Close']\n",
        "time_col = ['Date']\n",
        "\n",
        "# Input dimension\n",
        "input_dim = len(input_col)\n",
        "# Output dimension\n",
        "output_dim = len(output_col)\n",
        "\n",
        "# Number of session to prediction as one time\n",
        "prediction_size = 1\n",
        "# For each time model is train, the first is display\n",
        "sample_display_test_size = 5\n",
        "# Max bayer iteration\n",
        "bayer_max_evals = 100\n",
        "\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "Qk2bP_fgb8Hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Declare model\n",
        "# declare model\n",
        "def softMaxAxis1(x):\n",
        "    return softmax(x, axis=1)\n",
        "\n",
        "\n",
        "def get_model(input_dim, window_size, output_dim, lstm_layer_count=5, drop_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=100, input_shape=(window_size, input_dim), return_sequences=True))\n",
        "    model.add(Dropout(rate=drop_rate))\n",
        "\n",
        "    for i in range(lstm_layer_count - 2):\n",
        "        model.add(LSTM(units=100, return_sequences=True))\n",
        "        model.add(Dropout(rate=drop_rate))\n",
        "    \n",
        "    model.add(LSTM(output_dim, activation=softMaxAxis1))\n",
        "    opt = optimizers.Adam(lr=0.05, beta_1=0.99, beta_2=0.999)\n",
        "    model.compile(loss='MAE', optimizer='adam')\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shZCZlwbfSA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "y0p2kGBFb8Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Error metric\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "    return np.mean((y_true - y_pred) / y_true)\n",
        "\n",
        "\n",
        "def relative_root_mean_square_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    res = (y_true - y_pred) / y_true\n",
        "    res = np.power(res, 2)\n",
        "    res = np.mean(res)\n",
        "    res = math.sqrt(res)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "TQexGPe5b8H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Data preprocessing\n",
        "# reprocessing data\n",
        "def next_window(df, i, windows_size, prediction_size, input_col, output_col, time_col):\n",
        "    '''Generates the next data window from the given index location i'''\n",
        "    window = df[i: i + windows_size + prediction_size]\n",
        "    x = window[input_col][:-prediction_size]\n",
        "    y = window[output_col][-prediction_size:]\n",
        "    y_time = window[time_col][-prediction_size:]\n",
        "    return x, y, y_time\n",
        "\n",
        "def smooting_data(df, window_size):\n",
        "    return df.ewm(span=window_size).mean()\n",
        "\n",
        "def preprocessing_data(df, windows_size, prediction_size, input_col, output_col, time_col):\n",
        "    '''\n",
        "    Create x, y train data windows\n",
        "    Warning: batch method, not generative, make sure you have enough memory to\n",
        "    load data, otherwise use generate_training_window() method.\n",
        "    '''\n",
        "\n",
        "\n",
        "    data_x = []\n",
        "    data_y = []\n",
        "    data_y_time = []\n",
        "    for i in range(len(df) - windows_size - prediction_size):\n",
        "        x, y, y_time = next_window(df, i, windows_size, prediction_size, input_col, output_col, time_col)\n",
        "        data_x.append(x.values)\n",
        "        data_y.append(y.values)\n",
        "        data_y_time.append(y_time)\n",
        "\n",
        "    time = pd.concat(data_y_time)\n",
        "\n",
        "    return np.array(data_x), np.array(data_y), time.values\n",
        "\n",
        "\n",
        "def split_train_test_data(X, y):\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    return X_train, y_train, X_valid, y_valid\n",
        "\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "KdX8GHtvb8H9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Model train\n",
        "# Trainning model\n",
        "def train_model(model, X_train, y_train, X_valid, y_valid, stock_name, year, window_size):\n",
        "    if not IN_COLAB:\n",
        "        model_save_fname = os.path.join(model_dir, '%s-%s-w%d.h5' % (stock_name, year, window_size))\n",
        "    else:\n",
        "        model_save_fname = os.path.join(model_dir, '%s-%s-w%d-%s.h5' % (stock_name, year, window_size, datetime.now().strftime('%d%m%Y_%H%M%S')))\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=100),\n",
        "        ModelCheckpoint(filepath=model_save_fname, monitor='val_loss', save_best_only=True)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=1000,\n",
        "        batch_size=10000,\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "        shuffle=False)\n",
        "    model.save(model_save_fname)\n",
        "    \n",
        "    return history\n",
        "\n",
        "\n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "CKmceaTnb8ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# region Test model\n",
        "def test_model(model, test_data, window_size, prediction_size, input_col, output_col, time_col):\n",
        "    X, y, time = preprocessing_data(test_data, window_size, prediction_size, input_col, output_col, time_col)\n",
        "    \n",
        "    y_pred = model.predict(X)\n",
        "    \n",
        "    y_pred = np.repeat(y_pred, input_dim, axis=1)\n",
        "    y_pred = scaler.inverse_transform(y_pred)[:, [0]]\n",
        "    y_pred = pd.Series(y_pred.flatten())\n",
        "\n",
        "    df_test_result = pd.DataFrame(time, columns=['Date'])\n",
        "    df_test_result['Prediction'] = y_pred\n",
        "    df_test_result.set_index('Date', inplace=True)\n",
        "\n",
        "    return df_test_result\n",
        "\n",
        "\n",
        "def plot_test_result(test_result, stock_name, year, window_size):\n",
        "    # Plotly\n",
        "    trace0 = go.Scatter(\n",
        "        x=test_result.index,\n",
        "        y=test_result['Close'],\n",
        "        name='Thực tế',\n",
        "        line=dict(\n",
        "            color=('#5042f4'),\n",
        "            width=2)\n",
        "    )\n",
        "\n",
        "    trace1 = go.Scatter(\n",
        "        x=test_result.index,\n",
        "        y=test_result['Prediction'],\n",
        "        name='Dự đoán',\n",
        "        line=dict(\n",
        "            color=('#005b4e'),\n",
        "            width=2,\n",
        "            dash='dot'\n",
        "        )  # dash options include 'dash', 'dot', and 'dashdot'\n",
        "    )\n",
        "\n",
        "    data = [trace0, trace1]\n",
        "\n",
        "    # Edit the layout\n",
        "    layout = dict(title='Biểu đồ dự đoán',\n",
        "                  xaxis=dict(title='Date'),\n",
        "                  yaxis=dict(title='Price'),\n",
        "                  paper_bgcolor='#FFF9F5',\n",
        "                  plot_bgcolor='#FFF9F5'\n",
        "                  )\n",
        "\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    if not IN_COLAB:\n",
        "        fig.write_html(os.path.join(plot_dir, '%s_%s_w%d.html' % (stock_name, year, window_size)), auto_open=False)\n",
        "    else:\n",
        "        fig.write_html(os.path.join(plot_dir, '%s_%s_w%d_%s.html' % (stock_name, year, window_size, datetime.now().strftime('%d%m%Y_%H%M%S'))), auto_open=False)\n",
        "        \n",
        "# endregion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "A4S45MJvb8II",
        "colab_type": "code",
        "outputId": "3eeed349-0af3-4175-afb9-a965fe939ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# region Bayers\n",
        "def objective(params, df):\n",
        "    # Keep track of evals\n",
        "    global ITERATION\n",
        "\n",
        "    ITERATION += 1\n",
        "\n",
        "    # Make sure windows_size is int\n",
        "    windows_size = int(params['windows_size'])\n",
        "    print(f'Window size is {windows_size}')\n",
        "\n",
        "    model = get_model(input_dim, windows_size, output_dim)\n",
        "\n",
        "    start = timer()\n",
        "\n",
        "    # Handle data\n",
        "    # TODO: smoothing ddata\n",
        "    # df[input_col] = smooting_data(df[input_col], windows_size)\n",
        "\n",
        "    X, y, time = preprocessing_data(df, windows_size, prediction_size, input_col, output_col, time_col)\n",
        "\n",
        "    # Reshape data\n",
        "    y = y.reshape((y.shape[0], y.shape[1]))\n",
        "\n",
        "    X_train, y_train, X_valid, y_valid = split_train_test_data(X, y)\n",
        "\n",
        "    # Perform n_train\n",
        "    history = train_model(model, X_train, y_train, X_valid, y_valid, stock_name, year, windows_size)\n",
        "\n",
        "    run_time = timer() - start\n",
        "\n",
        "    # Test generated loss\n",
        "    test_result = test_model(model, df, windows_size, prediction_size, input_col, output_col, time_col)\n",
        "    test_result = test_result.join(df_org.set_index('Date'))\n",
        "    plot_test_result(test_result, stock_name, year, windows_size)\n",
        "\n",
        "    score = model.evaluate(X, y, 10000, 1)\n",
        "    print(f'Window size {windows_size} score = {score}')\n",
        "    #mae = mean_absolute_error(test_result['Close'], test_result['Prediction'])\n",
        "    #mse = mean_squared_error(test_result['Close'], test_result['Prediction'])\n",
        "    #mape = mean_absolute_percentage_error(test_result['Close'], test_result['Prediction'])\n",
        "    #rrmse = relative_root_mean_square_error(test_result['Close'], test_result['Prediction'])\n",
        "\n",
        "    #print(f'{stock_name} prediction for {prediction_size} day ahead')\n",
        "    #print(f'MAE = {mae}')\n",
        "    #print(f'MSE = {mse}')\n",
        "    #print(f'MAPE = {mape}')\n",
        "    #print(f'RRMSE = {rrmse}')\n",
        "    #loss = mape\n",
        "    loss = score\n",
        "    # write row\n",
        "    of_connection = open(result_save_fname, 'a')\n",
        "    writer = csv.writer(of_connection)\n",
        "    writer.writerow([stock_name, year, loss, params, ITERATION, windows_size, run_time])\n",
        "    of_connection.close()\n",
        "\n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'iteration': ITERATION, 'test_result': test_result,\n",
        "            'train_time': run_time, 'status': STATUS_OK}\n",
        "\n",
        "start_year = df_org['Date'].values[:1][0]\n",
        "start_year = pd.to_datetime(start_year).year\n",
        "\n",
        "end_year = df_org['Date'].values[-1:][0]\n",
        "end_year = pd.to_datetime(end_year).year\n",
        "\n",
        "windows_size_best = []\n",
        "# Global variable\n",
        "global ITERATION\n",
        "\n",
        "for year in range(start_year, end_year + 1):\n",
        "    df = df_org[df_org['Date'].dt.year == year].copy()\n",
        "\n",
        "    # Data too small, skip\n",
        "    if df.shape[0] < 10:\n",
        "        continue\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scaled_cols = scaler.fit_transform(df[input_col])\n",
        "    df[input_col] = scaled_cols\n",
        "\n",
        "    # Hyperparameter grid\n",
        "    param_grid = {\n",
        "        'windows_size': hp.choice('windows_size', np.arange(1, 8, dtype=int))\n",
        "    }\n",
        "\n",
        "    bayes_trials = Trials()\n",
        "\n",
        "    # Create the algorithm\n",
        "    bayes_algo = tpe.suggest\n",
        "\n",
        "    ITERATION = 0\n",
        "\n",
        "    fmin_objective = partial(objective, df=df)\n",
        "    bayes_best = fmin(fn=fmin_objective, space=param_grid,\n",
        "                      algo=bayes_algo, trials=bayes_trials,\n",
        "                      max_evals=bayer_max_evals)\n",
        "\n",
        "    of_connection = open(bayer_save_fname, 'a')\n",
        "    writer = csv.writer(of_connection)\n",
        "    writer.writerow([stock_name, year, bayes_best, '123'])\n",
        "    of_connection.close()\n",
        "\n",
        "    windows_size_best.append([year, bayes_best])\n",
        "# endregion\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Window size is 5\n",
            "  0%|          | 0/100 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1991-11-12</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.386182</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.390844</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1991-02-26</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.288209</td>\n",
              "      <td>0.275045</td>\n",
              "      <td>0.280458</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1991-05-28</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.048394</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.063215</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1991-12-17</td>\n",
              "      <td>1.873275</td>\n",
              "      <td>1.848351</td>\n",
              "      <td>1.873275</td>\n",
              "      <td>1.849503</td>\n",
              "      <td>1.873275</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1991-08-27</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.711360</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.721147</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1991-03-26</td>\n",
              "      <td>0.135995</td>\n",
              "      <td>0.155529</td>\n",
              "      <td>0.145370</td>\n",
              "      <td>0.154241</td>\n",
              "      <td>0.135995</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1991-03-12</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.232593</td>\n",
              "      <td>0.220689</td>\n",
              "      <td>0.202589</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1991-06-18</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.213839</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.227404</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1991-02-12</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.273612</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "45 1991-11-12  1.398926  1.386182  1.398926  1.390844   1.398926     0.0\n",
              "8  1991-02-26  0.254609  0.288209  0.275045  0.280458   0.254609     0.0\n",
              "21 1991-05-28  0.085853  0.048394  0.085853  0.063215   0.085853     0.0\n",
              "50 1991-12-17  1.873275  1.848351  1.873275  1.849503   1.873275     0.0\n",
              "34 1991-08-27  0.762667  0.711360  0.762667  0.721147   0.762667     0.0\n",
              "12 1991-03-26  0.135995  0.155529  0.145370  0.154241   0.135995     0.0\n",
              "10 1991-03-12  0.177921  0.232593  0.220689  0.202589   0.177921     0.0\n",
              "24 1991-06-18  0.256505  0.213839  0.256505  0.227404   0.256505     0.0\n",
              "6  1991-02-12  0.260297  0.260401  0.260297  0.273612   0.260297     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 37 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "  0%|          | 0/100 [00:04<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "37/37 [==============================]\n",
            " - 4s 102ms/step - loss: 0.2942 - val_loss: 1.0737\n",
            "\n",
            "Epoch 2/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2931 - val_loss: 1.0739\n",
            "\n",
            "Epoch 3/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2917 - val_loss: 1.0742\n",
            "\n",
            "Epoch 4/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2902 - val_loss: 1.0743\n",
            "\n",
            "Epoch 5/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2884 - val_loss: 1.0739\n",
            "\n",
            "Epoch 6/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2867 - val_loss: 1.0729\n",
            "\n",
            "Epoch 7/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2841 - val_loss: 1.0709\n",
            "\n",
            "Epoch 8/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2811 - val_loss: 1.0673\n",
            "\n",
            "Epoch 9/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2774 - val_loss: 1.0616\n",
            "\n",
            "Epoch 10/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2733 - val_loss: 1.0528\n",
            "\n",
            "Epoch 11/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2680 - val_loss: 1.0398\n",
            "\n",
            "Epoch 12/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2608 - val_loss: 1.0212\n",
            "\n",
            "Epoch 13/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2510 - val_loss: 0.9952\n",
            "\n",
            "Epoch 14/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2403 - val_loss: 0.9603\n",
            "\n",
            "Epoch 15/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2263 - val_loss: 0.9155\n",
            "\n",
            "Epoch 16/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2130 - val_loss: 0.8596\n",
            "\n",
            "Epoch 17/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1912 - val_loss: 0.7939\n",
            "\n",
            "Epoch 18/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1713 - val_loss: 0.7207\n",
            "\n",
            "Epoch 19/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1477 - val_loss: 0.6460\n",
            "\n",
            "Epoch 20/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1280 - val_loss: 0.5899\n",
            "\n",
            "Epoch 21/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1220 - val_loss: 0.5597\n",
            "\n",
            "Epoch 22/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1150 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1111 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1103 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1063 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1118 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1224 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1077 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1081 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1235 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1123 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1091 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1120 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1072 - val_loss: 0.5498\n",
            "\n",
            "Epoch 35/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1085 - val_loss: 0.5525\n",
            "\n",
            "Epoch 36/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0997 - val_loss: 0.5792\n",
            "\n",
            "Epoch 37/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1023 - val_loss: 0.6093\n",
            "\n",
            "Epoch 38/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.6395\n",
            "\n",
            "Epoch 39/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0897 - val_loss: 0.6676\n",
            "\n",
            "Epoch 40/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0987 - val_loss: 0.6950\n",
            "\n",
            "Epoch 41/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0975 - val_loss: 0.7129\n",
            "\n",
            "Epoch 42/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1002 - val_loss: 0.7279\n",
            "\n",
            "Epoch 43/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0995 - val_loss: 0.7404\n",
            "\n",
            "Epoch 44/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1042 - val_loss: 0.7474\n",
            "\n",
            "Epoch 45/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1004 - val_loss: 0.7489\n",
            "\n",
            "Epoch 46/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0925 - val_loss: 0.7460\n",
            "\n",
            "Epoch 47/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1029 - val_loss: 0.7412\n",
            "\n",
            "Epoch 48/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0976 - val_loss: 0.7315\n",
            "\n",
            "Epoch 49/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0974 - val_loss: 0.7179\n",
            "\n",
            "Epoch 50/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1005 - val_loss: 0.7060\n",
            "\n",
            "Epoch 51/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0972 - val_loss: 0.6906\n",
            "\n",
            "Epoch 52/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0977 - val_loss: 0.6751\n",
            "\n",
            "Epoch 53/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1053 - val_loss: 0.6605\n",
            "\n",
            "Epoch 54/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0969 - val_loss: 0.6539\n",
            "\n",
            "Epoch 55/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.6557\n",
            "\n",
            "Epoch 56/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0983 - val_loss: 0.6586\n",
            "\n",
            "Epoch 57/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0917 - val_loss: 0.6645\n",
            "\n",
            "Epoch 58/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0982 - val_loss: 0.6790\n",
            "\n",
            "Epoch 59/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0961 - val_loss: 0.6931\n",
            "\n",
            "Epoch 60/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0940 - val_loss: 0.7064\n",
            "\n",
            "Epoch 61/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0931 - val_loss: 0.7183\n",
            "\n",
            "Epoch 62/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0912 - val_loss: 0.7286\n",
            "\n",
            "Epoch 63/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1008 - val_loss: 0.7363\n",
            "\n",
            "Epoch 64/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0932 - val_loss: 0.7426\n",
            "\n",
            "Epoch 65/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0933 - val_loss: 0.7516\n",
            "\n",
            "Epoch 66/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0963 - val_loss: 0.7538\n",
            "\n",
            "Epoch 67/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0935 - val_loss: 0.7553\n",
            "\n",
            "Epoch 68/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0954 - val_loss: 0.7547\n",
            "\n",
            "Epoch 69/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0940 - val_loss: 0.7538\n",
            "\n",
            "Epoch 70/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0926 - val_loss: 0.7523\n",
            "\n",
            "Epoch 71/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0884 - val_loss: 0.7537\n",
            "\n",
            "Epoch 72/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0931 - val_loss: 0.7585\n",
            "\n",
            "Epoch 73/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0882 - val_loss: 0.7638\n",
            "\n",
            "Epoch 74/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0920 - val_loss: 0.7720\n",
            "\n",
            "Epoch 75/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0952 - val_loss: 0.7775\n",
            "\n",
            "Epoch 76/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7767\n",
            "\n",
            "Epoch 77/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0905 - val_loss: 0.7760\n",
            "\n",
            "Epoch 78/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0890 - val_loss: 0.7685\n",
            "\n",
            "Epoch 79/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0983 - val_loss: 0.7578\n",
            "\n",
            "Epoch 80/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0866 - val_loss: 0.7418\n",
            "\n",
            "Epoch 81/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0930 - val_loss: 0.7290\n",
            "\n",
            "Epoch 82/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0934 - val_loss: 0.7173\n",
            "\n",
            "Epoch 83/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0921 - val_loss: 0.7039\n",
            "\n",
            "Epoch 84/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0837 - val_loss: 0.6918\n",
            "\n",
            "Epoch 85/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0897 - val_loss: 0.6835\n",
            "\n",
            "Epoch 86/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0850 - val_loss: 0.6731\n",
            "\n",
            "Epoch 87/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0941 - val_loss: 0.6653\n",
            "\n",
            "Epoch 88/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0857 - val_loss: 0.6662\n",
            "\n",
            "Epoch 89/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0915 - val_loss: 0.6690\n",
            "\n",
            "Epoch 90/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0874 - val_loss: 0.6735\n",
            "\n",
            "Epoch 91/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0880 - val_loss: 0.6813\n",
            "\n",
            "Epoch 92/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0951 - val_loss: 0.6893\n",
            "\n",
            "Epoch 93/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0937 - val_loss: 0.6973\n",
            "\n",
            "Epoch 94/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0923 - val_loss: 0.7062\n",
            "\n",
            "Epoch 95/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0904 - val_loss: 0.7156\n",
            "\n",
            "Epoch 96/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0859 - val_loss: 0.7224\n",
            "\n",
            "Epoch 97/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0892 - val_loss: 0.7264\n",
            "\n",
            "Epoch 98/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0894 - val_loss: 0.7293\n",
            "\n",
            "Epoch 99/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0859 - val_loss: 0.7249\n",
            "\n",
            "Epoch 100/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0853 - val_loss: 0.7216\n",
            "\n",
            "Epoch 101/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0874 - val_loss: 0.7179\n",
            "\n",
            "Epoch 102/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0869 - val_loss: 0.7137\n",
            "\n",
            "Epoch 103/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0830 - val_loss: 0.7003\n",
            "\n",
            "Epoch 104/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0852 - val_loss: 0.6873\n",
            "\n",
            "Epoch 105/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0916 - val_loss: 0.6739\n",
            "\n",
            "Epoch 106/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0931 - val_loss: 0.6676\n",
            "\n",
            "Epoch 107/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0890 - val_loss: 0.6630\n",
            "\n",
            "Epoch 108/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0875 - val_loss: 0.6648\n",
            "\n",
            "Epoch 109/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0945 - val_loss: 0.6662\n",
            "\n",
            "Epoch 110/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0934 - val_loss: 0.6683\n",
            "\n",
            "Epoch 111/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0875 - val_loss: 0.6676\n",
            "\n",
            "Epoch 112/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0792 - val_loss: 0.6663\n",
            "\n",
            "Epoch 113/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0929 - val_loss: 0.6707\n",
            "\n",
            "Epoch 114/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0814 - val_loss: 0.6742\n",
            "\n",
            "Epoch 115/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0819 - val_loss: 0.6762\n",
            "\n",
            "Epoch 116/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0875 - val_loss: 0.6821\n",
            "\n",
            "Epoch 117/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0820 - val_loss: 0.6903\n",
            "\n",
            "Epoch 118/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0822 - val_loss: 0.6967\n",
            "\n",
            "Epoch 119/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0824 - val_loss: 0.7021\n",
            "\n",
            "Epoch 120/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0893 - val_loss: 0.7013\n",
            "\n",
            "Epoch 121/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0853 - val_loss: 0.6996\n",
            "\n",
            "Epoch 122/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0849 - val_loss: 0.6925\n",
            "\n",
            "47/47 [==============================]\n",
            " - 0s 205us/step\n",
            "\n",
            "Window size 5 score = 0.21078190207481384\n",
            "Window size is 6\n",
            "  1%|          | 1/100 [00:23<38:18, 23.21s/it, best loss: 0.21078190207481384]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1991-01-15</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.296939</td>\n",
              "      <td>0.283577</td>\n",
              "      <td>0.304525</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991-01-08</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.274736</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.287838</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1991-07-02</td>\n",
              "      <td>0.289266</td>\n",
              "      <td>0.310088</td>\n",
              "      <td>0.303487</td>\n",
              "      <td>0.315542</td>\n",
              "      <td>0.289266</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1991-10-08</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.865488</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.874104</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1991-05-21</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1991-04-09</td>\n",
              "      <td>0.115348</td>\n",
              "      <td>0.144751</td>\n",
              "      <td>0.134836</td>\n",
              "      <td>0.139052</td>\n",
              "      <td>0.115348</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1991-03-26</td>\n",
              "      <td>0.135995</td>\n",
              "      <td>0.155529</td>\n",
              "      <td>0.145370</td>\n",
              "      <td>0.154241</td>\n",
              "      <td>0.135995</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1991-12-31</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1991-11-12</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.386182</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.390844</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "2  1991-01-15  0.278310  0.296939  0.283577  0.304525   0.278310     0.0\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "1  1991-01-08  0.282840  0.274736  0.282840  0.287838   0.282840     0.0\n",
              "26 1991-07-02  0.289266  0.310088  0.303487  0.315542   0.289266     0.0\n",
              "40 1991-10-08  0.914885  0.865488  0.914885  0.874104   0.914885     0.0\n",
              "20 1991-05-21  0.029917  0.017461  0.029917  0.026527   0.029917     0.0\n",
              "14 1991-04-09  0.115348  0.144751  0.134836  0.139052   0.115348     0.0\n",
              "12 1991-03-26  0.135995  0.155529  0.145370  0.154241   0.135995     0.0\n",
              "52 1991-12-31  2.000000  2.000000  2.000000  2.000000   2.000000     0.0\n",
              "45 1991-11-12  1.398926  1.386182  1.398926  1.390844   1.398926     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "36/36 [==============================]\n",
            " - 5s 139ms/step - loss: 0.2807 - val_loss: 1.1119\n",
            "\n",
            "Epoch 2/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2796 - val_loss: 1.1111\n",
            "\n",
            "Epoch 3/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2783 - val_loss: 1.1102\n",
            "\n",
            "Epoch 4/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2769 - val_loss: 1.1088\n",
            "\n",
            "Epoch 5/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2754 - val_loss: 1.1065\n",
            "\n",
            "Epoch 6/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2732 - val_loss: 1.1027\n",
            "\n",
            "Epoch 7/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2709 - val_loss: 1.0966\n",
            "\n",
            "Epoch 8/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2675 - val_loss: 1.0873\n",
            "\n",
            "Epoch 9/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2636 - val_loss: 1.0734\n",
            "\n",
            "Epoch 10/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2588 - val_loss: 1.0535\n",
            "\n",
            "Epoch 11/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2532 - val_loss: 1.0255\n",
            "\n",
            "Epoch 12/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2445 - val_loss: 0.9875\n",
            "\n",
            "Epoch 13/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2347 - val_loss: 0.9371\n",
            "\n",
            "Epoch 14/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2217 - val_loss: 0.8736\n",
            "\n",
            "Epoch 15/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2037 - val_loss: 0.7985\n",
            "\n",
            "Epoch 16/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1855 - val_loss: 0.7137\n",
            "\n",
            "Epoch 17/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1665 - val_loss: 0.6264\n",
            "\n",
            "Epoch 18/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1464 - val_loss: 0.5680\n",
            "\n",
            "Epoch 19/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1348 - val_loss: 0.5498\n",
            "\n",
            "Epoch 20/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1363 - val_loss: 0.5498\n",
            "\n",
            "Epoch 21/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1335 - val_loss: 0.5498\n",
            "\n",
            "Epoch 22/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1254 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1278 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1251 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1227 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1272 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1216 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1144 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1289 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1150 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1251 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1429 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1242 - val_loss: 0.5498\n",
            "\n",
            "Epoch 35/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1161 - val_loss: 0.5498\n",
            "\n",
            "Epoch 36/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1202 - val_loss: 0.5533\n",
            "\n",
            "Epoch 37/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1224 - val_loss: 0.5826\n",
            "\n",
            "Epoch 38/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1067 - val_loss: 0.6178\n",
            "\n",
            "Epoch 39/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1153 - val_loss: 0.6570\n",
            "\n",
            "Epoch 40/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1077 - val_loss: 0.6911\n",
            "\n",
            "Epoch 41/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1139 - val_loss: 0.7211\n",
            "\n",
            "Epoch 42/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1108 - val_loss: 0.7441\n",
            "\n",
            "Epoch 43/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1110 - val_loss: 0.7567\n",
            "\n",
            "Epoch 44/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1067 - val_loss: 0.7626\n",
            "\n",
            "Epoch 45/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1126 - val_loss: 0.7677\n",
            "\n",
            "Epoch 46/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1220 - val_loss: 0.7638\n",
            "\n",
            "Epoch 47/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1094 - val_loss: 0.7590\n",
            "\n",
            "Epoch 48/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1087 - val_loss: 0.7500\n",
            "\n",
            "Epoch 49/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1099 - val_loss: 0.7329\n",
            "\n",
            "Epoch 50/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1047 - val_loss: 0.7161\n",
            "\n",
            "Epoch 51/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1026 - val_loss: 0.6960\n",
            "\n",
            "Epoch 52/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1042 - val_loss: 0.6718\n",
            "\n",
            "Epoch 53/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1015 - val_loss: 0.6443\n",
            "\n",
            "Epoch 54/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1038 - val_loss: 0.6271\n",
            "\n",
            "Epoch 55/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1020 - val_loss: 0.6156\n",
            "\n",
            "Epoch 56/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1052 - val_loss: 0.6068\n",
            "\n",
            "Epoch 57/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1017 - val_loss: 0.6041\n",
            "\n",
            "Epoch 58/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1069 - val_loss: 0.6107\n",
            "\n",
            "Epoch 59/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1026 - val_loss: 0.6242\n",
            "\n",
            "Epoch 60/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0972 - val_loss: 0.6432\n",
            "\n",
            "Epoch 61/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1018 - val_loss: 0.6642\n",
            "\n",
            "Epoch 62/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1039 - val_loss: 0.6865\n",
            "\n",
            "Epoch 63/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1052 - val_loss: 0.7098\n",
            "\n",
            "Epoch 64/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1003 - val_loss: 0.7300\n",
            "\n",
            "Epoch 65/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1083 - val_loss: 0.7435\n",
            "\n",
            "Epoch 66/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1075 - val_loss: 0.7513\n",
            "\n",
            "Epoch 67/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1027 - val_loss: 0.7573\n",
            "\n",
            "Epoch 68/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0984 - val_loss: 0.7548\n",
            "\n",
            "Epoch 69/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1043 - val_loss: 0.7482\n",
            "\n",
            "Epoch 70/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1004 - val_loss: 0.7381\n",
            "\n",
            "Epoch 71/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0982 - val_loss: 0.7283\n",
            "\n",
            "Epoch 72/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0958 - val_loss: 0.7153\n",
            "\n",
            "Epoch 73/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0999 - val_loss: 0.6987\n",
            "\n",
            "Epoch 74/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0998 - val_loss: 0.6769\n",
            "\n",
            "Epoch 75/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0940 - val_loss: 0.6581\n",
            "\n",
            "Epoch 76/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0979 - val_loss: 0.6437\n",
            "\n",
            "Epoch 77/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0989 - val_loss: 0.6324\n",
            "\n",
            "Epoch 78/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1109 - val_loss: 0.6236\n",
            "\n",
            "Epoch 79/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0997 - val_loss: 0.6190\n",
            "\n",
            "Epoch 80/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0979 - val_loss: 0.6237\n",
            "\n",
            "Epoch 81/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1021 - val_loss: 0.6357\n",
            "\n",
            "Epoch 82/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0961 - val_loss: 0.6522\n",
            "\n",
            "Epoch 83/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1033 - val_loss: 0.6659\n",
            "\n",
            "Epoch 84/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1007 - val_loss: 0.6778\n",
            "\n",
            "Epoch 85/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0990 - val_loss: 0.6900\n",
            "\n",
            "Epoch 86/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0958 - val_loss: 0.6968\n",
            "\n",
            "Epoch 87/1000\n",
            "36/36 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0925 - val_loss: 0.7078\n",
            "\n",
            "Epoch 88/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0950 - val_loss: 0.7188\n",
            "\n",
            "Epoch 89/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1052 - val_loss: 0.7284\n",
            "\n",
            "Epoch 90/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0977 - val_loss: 0.7334\n",
            "\n",
            "Epoch 91/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0967 - val_loss: 0.7310\n",
            "\n",
            "Epoch 92/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1094 - val_loss: 0.7258\n",
            "\n",
            "Epoch 93/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0944 - val_loss: 0.7180\n",
            "\n",
            "Epoch 94/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0955 - val_loss: 0.7055\n",
            "\n",
            "Epoch 95/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1012 - val_loss: 0.6977\n",
            "\n",
            "Epoch 96/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1033 - val_loss: 0.6893\n",
            "\n",
            "Epoch 97/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0934 - val_loss: 0.6827\n",
            "\n",
            "Epoch 98/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0984 - val_loss: 0.6811\n",
            "\n",
            "Epoch 99/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0947 - val_loss: 0.6767\n",
            "\n",
            "Epoch 100/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0990 - val_loss: 0.6735\n",
            "\n",
            "Epoch 101/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0928 - val_loss: 0.6735\n",
            "\n",
            "Epoch 102/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0923 - val_loss: 0.6703\n",
            "\n",
            "Epoch 103/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0958 - val_loss: 0.6688\n",
            "\n",
            "Epoch 104/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0966 - val_loss: 0.6731\n",
            "\n",
            "Epoch 105/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0909 - val_loss: 0.6780\n",
            "\n",
            "Epoch 106/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0967 - val_loss: 0.6867\n",
            "\n",
            "Epoch 107/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0940 - val_loss: 0.6917\n",
            "\n",
            "Epoch 108/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0964 - val_loss: 0.6994\n",
            "\n",
            "Epoch 109/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1016 - val_loss: 0.7032\n",
            "\n",
            "Epoch 110/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0970 - val_loss: 0.7059\n",
            "\n",
            "Epoch 111/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1028 - val_loss: 0.7054\n",
            "\n",
            "Epoch 112/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.7016\n",
            "\n",
            "Epoch 113/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0948 - val_loss: 0.7014\n",
            "\n",
            "Epoch 114/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0986 - val_loss: 0.6974\n",
            "\n",
            "Epoch 115/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0944 - val_loss: 0.6921\n",
            "\n",
            "Epoch 116/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0955 - val_loss: 0.6850\n",
            "\n",
            "Epoch 117/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0969 - val_loss: 0.6806\n",
            "\n",
            "Epoch 118/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0939 - val_loss: 0.6799\n",
            "\n",
            "Epoch 119/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0928 - val_loss: 0.6789\n",
            "\n",
            "46/46 [==============================]\n",
            " - 0s 231us/step\n",
            "\n",
            "Window size 6 score = 0.2204994112253189\n",
            "Window size is 1\n",
            "  2%|▏         | 2/100 [00:49<39:10, 23.98s/it, best loss: 0.21078190207481384]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1991-05-28</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.048394</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.063215</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1991-07-23</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.347058</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.359611</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1991-12-10</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.786161</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.787785</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1991-11-26</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.571244</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.574500</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1991-03-12</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.232593</td>\n",
              "      <td>0.220689</td>\n",
              "      <td>0.202589</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1991-01-01</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1991-11-12</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.386182</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>1.390844</td>\n",
              "      <td>1.398926</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1991-01-15</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.296939</td>\n",
              "      <td>0.283577</td>\n",
              "      <td>0.304525</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1991-05-21</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "21 1991-05-28  0.085853  0.048394  0.085853  0.063215   0.085853     0.0\n",
              "29 1991-07-23  0.354998  0.347058  0.354998  0.359611   0.354998     0.0\n",
              "49 1991-12-10  1.791109  1.786161  1.791109  1.787785   1.791109     0.0\n",
              "47 1991-11-26  1.626146  1.571244  1.626146  1.574500   1.626146     0.0\n",
              "10 1991-03-12  0.177921  0.232593  0.220689  0.202589   0.177921     0.0\n",
              "19 1991-05-14  0.000000  0.000000  0.000000  0.000000   0.000000     0.0\n",
              "0  1991-01-01  0.255346  0.233348  0.255346  0.246764   0.255346     0.0\n",
              "45 1991-11-12  1.398926  1.386182  1.398926  1.390844   1.398926     0.0\n",
              "2  1991-01-15  0.278310  0.296939  0.283577  0.304525   0.278310     0.0\n",
              "20 1991-05-21  0.029917  0.017461  0.029917  0.026527   0.029917     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 40 samples, validate on 11 samples\n",
            "Epoch 1/1000\n",
            "40/40 [==============================]\n",
            " - 6s 145ms/step - loss: 0.2917 - val_loss: 1.0024\n",
            "\n",
            "Epoch 2/1000\n",
            "40/40 [==============================]\n",
            " - 0s 782us/step - loss: 0.2913 - val_loss: 1.0032\n",
            "\n",
            "Epoch 3/1000\n",
            "40/40 [==============================]\n",
            " - 0s 781us/step - loss: 0.2908 - val_loss: 1.0041\n",
            "\n",
            "Epoch 4/1000\n",
            "40/40 [==============================]\n",
            " - 0s 788us/step - loss: 0.2904 - val_loss: 1.0050\n",
            "\n",
            "Epoch 5/1000\n",
            "40/40 [==============================]\n",
            " - 0s 796us/step - loss: 0.2899 - val_loss: 1.0060\n",
            "\n",
            "Epoch 6/1000\n",
            "40/40 [==============================]\n",
            " - 0s 823us/step - loss: 0.2894 - val_loss: 1.0070\n",
            "\n",
            "Epoch 7/1000\n",
            "40/40 [==============================]\n",
            " - 0s 756us/step - loss: 0.2889 - val_loss: 1.0080\n",
            "\n",
            "Epoch 8/1000\n",
            "40/40 [==============================]\n",
            " - 0s 532us/step - loss: 0.2883 - val_loss: 1.0090\n",
            "\n",
            "Epoch 9/1000\n",
            "40/40 [==============================]\n",
            " - 0s 780us/step - loss: 0.2877 - val_loss: 1.0101\n",
            "\n",
            "Epoch 10/1000\n",
            "40/40 [==============================]\n",
            " - 0s 846us/step - loss: 0.2871 - val_loss: 1.0113\n",
            "\n",
            "Epoch 11/1000\n",
            "40/40 [==============================]\n",
            " - 0s 698us/step - loss: 0.2865 - val_loss: 1.0124\n",
            "\n",
            "Epoch 12/1000\n",
            "40/40 [==============================]\n",
            " - 0s 854us/step - loss: 0.2860 - val_loss: 1.0136\n",
            "\n",
            "Epoch 13/1000\n",
            "40/40 [==============================]\n",
            " - 0s 819us/step - loss: 0.2852 - val_loss: 1.0149\n",
            "\n",
            "Epoch 14/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2843 - val_loss: 1.0162\n",
            "\n",
            "Epoch 15/1000\n",
            "40/40 [==============================]\n",
            " - 0s 829us/step - loss: 0.2836 - val_loss: 1.0176\n",
            "\n",
            "Epoch 16/1000\n",
            "40/40 [==============================]\n",
            " - 0s 906us/step - loss: 0.2828 - val_loss: 1.0190\n",
            "\n",
            "Epoch 17/1000\n",
            "40/40 [==============================]\n",
            " - 0s 681us/step - loss: 0.2822 - val_loss: 1.0204\n",
            "\n",
            "Epoch 18/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2812 - val_loss: 1.0219\n",
            "\n",
            "Epoch 19/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2801 - val_loss: 1.0234\n",
            "\n",
            "Epoch 20/1000\n",
            "40/40 [==============================]\n",
            " - 0s 773us/step - loss: 0.2787 - val_loss: 1.0249\n",
            "\n",
            "Epoch 21/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2779 - val_loss: 1.0263\n",
            "\n",
            "Epoch 22/1000\n",
            "40/40 [==============================]\n",
            " - 0s 751us/step - loss: 0.2766 - val_loss: 1.0278\n",
            "\n",
            "Epoch 23/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2753 - val_loss: 1.0292\n",
            "\n",
            "Epoch 24/1000\n",
            "40/40 [==============================]\n",
            " - 0s 976us/step - loss: 0.2738 - val_loss: 1.0305\n",
            "\n",
            "Epoch 25/1000\n",
            "40/40 [==============================]\n",
            " - 0s 849us/step - loss: 0.2718 - val_loss: 1.0318\n",
            "\n",
            "Epoch 26/1000\n",
            "40/40 [==============================]\n",
            " - 0s 970us/step - loss: 0.2701 - val_loss: 1.0329\n",
            "\n",
            "Epoch 27/1000\n",
            "40/40 [==============================]\n",
            " - 0s 878us/step - loss: 0.2684 - val_loss: 1.0337\n",
            "\n",
            "Epoch 28/1000\n",
            "40/40 [==============================]\n",
            " - 0s 884us/step - loss: 0.2665 - val_loss: 1.0343\n",
            "\n",
            "Epoch 29/1000\n",
            "40/40 [==============================]\n",
            " - 0s 844us/step - loss: 0.2633 - val_loss: 1.0346\n",
            "\n",
            "Epoch 30/1000\n",
            "40/40 [==============================]\n",
            " - 0s 892us/step - loss: 0.2614 - val_loss: 1.0345\n",
            "\n",
            "Epoch 31/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2591 - val_loss: 1.0339\n",
            "\n",
            "Epoch 32/1000\n",
            "40/40 [==============================]\n",
            " - 0s 908us/step - loss: 0.2561 - val_loss: 1.0327\n",
            "\n",
            "Epoch 33/1000\n",
            "40/40 [==============================]\n",
            " - 0s 862us/step - loss: 0.2527 - val_loss: 1.0309\n",
            "\n",
            "Epoch 34/1000\n",
            "40/40 [==============================]\n",
            " - 0s 922us/step - loss: 0.2492 - val_loss: 1.0282\n",
            "\n",
            "Epoch 35/1000\n",
            "40/40 [==============================]\n",
            " - 0s 766us/step - loss: 0.2445 - val_loss: 1.0244\n",
            "\n",
            "Epoch 36/1000\n",
            "40/40 [==============================]\n",
            " - 0s 584us/step - loss: 0.2404 - val_loss: 1.0192\n",
            "\n",
            "Epoch 37/1000\n",
            "40/40 [==============================]\n",
            " - 0s 858us/step - loss: 0.2361 - val_loss: 1.0125\n",
            "\n",
            "Epoch 38/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2316 - val_loss: 1.0043\n",
            "\n",
            "Epoch 39/1000\n",
            "40/40 [==============================]\n",
            " - 0s 785us/step - loss: 0.2251 - val_loss: 0.9942\n",
            "\n",
            "Epoch 40/1000\n",
            "40/40 [==============================]\n",
            " - 0s 829us/step - loss: 0.2176 - val_loss: 0.9821\n",
            "\n",
            "Epoch 41/1000\n",
            "40/40 [==============================]\n",
            " - 0s 773us/step - loss: 0.2114 - val_loss: 0.9679\n",
            "\n",
            "Epoch 42/1000\n",
            "40/40 [==============================]\n",
            " - 0s 742us/step - loss: 0.2040 - val_loss: 0.9514\n",
            "\n",
            "Epoch 43/1000\n",
            "40/40 [==============================]\n",
            " - 0s 780us/step - loss: 0.1939 - val_loss: 0.9325\n",
            "\n",
            "Epoch 44/1000\n",
            "40/40 [==============================]\n",
            " - 0s 742us/step - loss: 0.1825 - val_loss: 0.9112\n",
            "\n",
            "Epoch 45/1000\n",
            "40/40 [==============================]\n",
            " - 0s 779us/step - loss: 0.1724 - val_loss: 0.8875\n",
            "\n",
            "Epoch 46/1000\n",
            "40/40 [==============================]\n",
            " - 0s 600us/step - loss: 0.1603 - val_loss: 0.8617\n",
            "\n",
            "Epoch 47/1000\n",
            "40/40 [==============================]\n",
            " - 0s 714us/step - loss: 0.1508 - val_loss: 0.8341\n",
            "\n",
            "Epoch 48/1000\n",
            "40/40 [==============================]\n",
            " - 0s 858us/step - loss: 0.1429 - val_loss: 0.8052\n",
            "\n",
            "Epoch 49/1000\n",
            "40/40 [==============================]\n",
            " - 0s 495us/step - loss: 0.1290 - val_loss: 0.7760\n",
            "\n",
            "Epoch 50/1000\n",
            "40/40 [==============================]\n",
            " - 0s 595us/step - loss: 0.1110 - val_loss: 0.7466\n",
            "\n",
            "Epoch 51/1000\n",
            "40/40 [==============================]\n",
            " - 0s 574us/step - loss: 0.1052 - val_loss: 0.7180\n",
            "\n",
            "Epoch 52/1000\n",
            "40/40 [==============================]\n",
            " - 0s 794us/step - loss: 0.0971 - val_loss: 0.6905\n",
            "\n",
            "Epoch 53/1000\n",
            "40/40 [==============================]\n",
            " - 0s 573us/step - loss: 0.0879 - val_loss: 0.6647\n",
            "\n",
            "Epoch 54/1000\n",
            "40/40 [==============================]\n",
            " - 0s 601us/step - loss: 0.0804 - val_loss: 0.6410\n",
            "\n",
            "Epoch 55/1000\n",
            "40/40 [==============================]\n",
            " - 0s 733us/step - loss: 0.0745 - val_loss: 0.6196\n",
            "\n",
            "Epoch 56/1000\n",
            "40/40 [==============================]\n",
            " - 0s 757us/step - loss: 0.0661 - val_loss: 0.6002\n",
            "\n",
            "Epoch 57/1000\n",
            "40/40 [==============================]\n",
            " - 0s 691us/step - loss: 0.0593 - val_loss: 0.5831\n",
            "\n",
            "Epoch 58/1000\n",
            "40/40 [==============================]\n",
            " - 0s 743us/step - loss: 0.0485 - val_loss: 0.5694\n",
            "\n",
            "Epoch 59/1000\n",
            "40/40 [==============================]\n",
            " - 0s 725us/step - loss: 0.0440 - val_loss: 0.5576\n",
            "\n",
            "Epoch 60/1000\n",
            "40/40 [==============================]\n",
            " - 0s 644us/step - loss: 0.0439 - val_loss: 0.5480\n",
            "\n",
            "Epoch 61/1000\n",
            "40/40 [==============================]\n",
            " - 0s 741us/step - loss: 0.0486 - val_loss: 0.5430\n",
            "\n",
            "Epoch 62/1000\n",
            "40/40 [==============================]\n",
            " - 0s 517us/step - loss: 0.0564 - val_loss: 0.5424\n",
            "\n",
            "Epoch 63/1000\n",
            "40/40 [==============================]\n",
            " - 0s 659us/step - loss: 0.0577 - val_loss: 0.5452\n",
            "\n",
            "Epoch 64/1000\n",
            "40/40 [==============================]\n",
            " - 0s 733us/step - loss: 0.0548 - val_loss: 0.5508\n",
            "\n",
            "Epoch 65/1000\n",
            "40/40 [==============================]\n",
            " - 0s 801us/step - loss: 0.0583 - val_loss: 0.5586\n",
            "\n",
            "Epoch 66/1000\n",
            "40/40 [==============================]\n",
            " - 0s 826us/step - loss: 0.0480 - val_loss: 0.5670\n",
            "\n",
            "Epoch 67/1000\n",
            "40/40 [==============================]\n",
            " - 0s 717us/step - loss: 0.0486 - val_loss: 0.5761\n",
            "\n",
            "Epoch 68/1000\n",
            "40/40 [==============================]\n",
            " - 0s 744us/step - loss: 0.0457 - val_loss: 0.5836\n",
            "\n",
            "Epoch 69/1000\n",
            "40/40 [==============================]\n",
            " - 0s 690us/step - loss: 0.0471 - val_loss: 0.5916\n",
            "\n",
            "Epoch 70/1000\n",
            "40/40 [==============================]\n",
            " - 0s 941us/step - loss: 0.0474 - val_loss: 0.5978\n",
            "\n",
            "Epoch 71/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0462 - val_loss: 0.6035\n",
            "\n",
            "Epoch 72/1000\n",
            "40/40 [==============================]\n",
            " - 0s 916us/step - loss: 0.0443 - val_loss: 0.6087\n",
            "\n",
            "Epoch 73/1000\n",
            "40/40 [==============================]\n",
            " - 0s 985us/step - loss: 0.0497 - val_loss: 0.6128\n",
            "\n",
            "Epoch 74/1000\n",
            "40/40 [==============================]\n",
            " - 0s 993us/step - loss: 0.0472 - val_loss: 0.6158\n",
            "\n",
            "Epoch 75/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0483 - val_loss: 0.6175\n",
            "\n",
            "Epoch 76/1000\n",
            "40/40 [==============================]\n",
            " - 0s 869us/step - loss: 0.0398 - val_loss: 0.6187\n",
            "\n",
            "Epoch 77/1000\n",
            "40/40 [==============================]\n",
            " - 0s 885us/step - loss: 0.0439 - val_loss: 0.6184\n",
            "\n",
            "Epoch 78/1000\n",
            "40/40 [==============================]\n",
            " - 0s 860us/step - loss: 0.0532 - val_loss: 0.6170\n",
            "\n",
            "Epoch 79/1000\n",
            "40/40 [==============================]\n",
            " - 0s 838us/step - loss: 0.0462 - val_loss: 0.6146\n",
            "\n",
            "Epoch 80/1000\n",
            "40/40 [==============================]\n",
            " - 0s 550us/step - loss: 0.0425 - val_loss: 0.6107\n",
            "\n",
            "Epoch 81/1000\n",
            "40/40 [==============================]\n",
            " - 0s 577us/step - loss: 0.0426 - val_loss: 0.6056\n",
            "\n",
            "Epoch 82/1000\n",
            "40/40 [==============================]\n",
            " - 0s 620us/step - loss: 0.0417 - val_loss: 0.6005\n",
            "\n",
            "Epoch 83/1000\n",
            "40/40 [==============================]\n",
            " - 0s 573us/step - loss: 0.0405 - val_loss: 0.5956\n",
            "\n",
            "Epoch 84/1000\n",
            "40/40 [==============================]\n",
            " - 0s 578us/step - loss: 0.0468 - val_loss: 0.5903\n",
            "\n",
            "Epoch 85/1000\n",
            "40/40 [==============================]\n",
            " - 0s 946us/step - loss: 0.0459 - val_loss: 0.5846\n",
            "\n",
            "Epoch 86/1000\n",
            "40/40 [==============================]\n",
            " - 0s 671us/step - loss: 0.0398 - val_loss: 0.5809\n",
            "\n",
            "Epoch 87/1000\n",
            "40/40 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0417 - val_loss: 0.5784\n",
            "\n",
            "Epoch 88/1000\n",
            "40/40 [==============================]\n",
            " - 0s 597us/step - loss: 0.0401 - val_loss: 0.5767\n",
            "\n",
            "Epoch 89/1000\n",
            "40/40 [==============================]\n",
            " - 0s 643us/step - loss: 0.0447 - val_loss: 0.5762\n",
            "\n",
            "Epoch 90/1000\n",
            "40/40 [==============================]\n",
            " - 0s 666us/step - loss: 0.0417 - val_loss: 0.5770\n",
            "\n",
            "Epoch 91/1000\n",
            "40/40 [==============================]\n",
            " - 0s 857us/step - loss: 0.0393 - val_loss: 0.5778\n",
            "\n",
            "Epoch 92/1000\n",
            "40/40 [==============================]\n",
            " - 0s 766us/step - loss: 0.0429 - val_loss: 0.5781\n",
            "\n",
            "Epoch 93/1000\n",
            "40/40 [==============================]\n",
            " - 0s 769us/step - loss: 0.0383 - val_loss: 0.5800\n",
            "\n",
            "Epoch 94/1000\n",
            "40/40 [==============================]\n",
            " - 0s 799us/step - loss: 0.0402 - val_loss: 0.5812\n",
            "\n",
            "Epoch 95/1000\n",
            "40/40 [==============================]\n",
            " - 0s 919us/step - loss: 0.0453 - val_loss: 0.5820\n",
            "\n",
            "Epoch 96/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0485 - val_loss: 0.5834\n",
            "\n",
            "Epoch 97/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0513 - val_loss: 0.5851\n",
            "\n",
            "Epoch 98/1000\n",
            "40/40 [==============================]\n",
            " - 0s 897us/step - loss: 0.0454 - val_loss: 0.5860\n",
            "\n",
            "Epoch 99/1000\n",
            "40/40 [==============================]\n",
            " - 0s 741us/step - loss: 0.0383 - val_loss: 0.5872\n",
            "\n",
            "Epoch 100/1000\n",
            "40/40 [==============================]\n",
            " - 0s 994us/step - loss: 0.0421 - val_loss: 0.5877\n",
            "\n",
            "Epoch 101/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0451 - val_loss: 0.5879\n",
            "\n",
            "Epoch 102/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0405 - val_loss: 0.5887\n",
            "\n",
            "Epoch 103/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0381 - val_loss: 0.5901\n",
            "\n",
            "Epoch 104/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0398 - val_loss: 0.5907\n",
            "\n",
            "Epoch 105/1000\n",
            "40/40 [==============================]\n",
            " - 0s 943us/step - loss: 0.0390 - val_loss: 0.5909\n",
            "\n",
            "Epoch 106/1000\n",
            "40/40 [==============================]\n",
            " - 0s 987us/step - loss: 0.0376 - val_loss: 0.5907\n",
            "\n",
            "Epoch 107/1000\n",
            "40/40 [==============================]\n",
            " - 0s 841us/step - loss: 0.0414 - val_loss: 0.5910\n",
            "\n",
            "Epoch 108/1000\n",
            "40/40 [==============================]\n",
            " - 0s 768us/step - loss: 0.0399 - val_loss: 0.5910\n",
            "\n",
            "Epoch 109/1000\n",
            "40/40 [==============================]\n",
            " - 0s 952us/step - loss: 0.0481 - val_loss: 0.5918\n",
            "\n",
            "Epoch 110/1000\n",
            "40/40 [==============================]\n",
            " - 0s 979us/step - loss: 0.0419 - val_loss: 0.5924\n",
            "\n",
            "Epoch 111/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0390 - val_loss: 0.5923\n",
            "\n",
            "Epoch 112/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0404 - val_loss: 0.5914\n",
            "\n",
            "Epoch 113/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0455 - val_loss: 0.5910\n",
            "\n",
            "Epoch 114/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0371 - val_loss: 0.5912\n",
            "\n",
            "Epoch 115/1000\n",
            "40/40 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0419 - val_loss: 0.5920\n",
            "\n",
            "Epoch 116/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0382 - val_loss: 0.5923\n",
            "\n",
            "Epoch 117/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0372 - val_loss: 0.5921\n",
            "\n",
            "Epoch 118/1000\n",
            "40/40 [==============================]\n",
            " - 0s 786us/step - loss: 0.0412 - val_loss: 0.5913\n",
            "\n",
            "Epoch 119/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0421 - val_loss: 0.5901\n",
            "\n",
            "Epoch 120/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0406 - val_loss: 0.5888\n",
            "\n",
            "Epoch 121/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0411 - val_loss: 0.5865\n",
            "\n",
            "Epoch 122/1000\n",
            "40/40 [==============================]\n",
            " - 0s 831us/step - loss: 0.0391 - val_loss: 0.5844\n",
            "\n",
            "Epoch 123/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0395 - val_loss: 0.5835\n",
            "\n",
            "Epoch 124/1000\n",
            "40/40 [==============================]\n",
            " - 0s 947us/step - loss: 0.0376 - val_loss: 0.5832\n",
            "\n",
            "Epoch 125/1000\n",
            "40/40 [==============================]\n",
            " - 0s 857us/step - loss: 0.0392 - val_loss: 0.5835\n",
            "\n",
            "Epoch 126/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0436 - val_loss: 0.5848\n",
            "\n",
            "Epoch 127/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0400 - val_loss: 0.5879\n",
            "\n",
            "Epoch 128/1000\n",
            "40/40 [==============================]\n",
            " - 0s 905us/step - loss: 0.0398 - val_loss: 0.5904\n",
            "\n",
            "Epoch 129/1000\n",
            "40/40 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0416 - val_loss: 0.5926\n",
            "\n",
            "Epoch 130/1000\n",
            "40/40 [==============================]\n",
            " - 0s 986us/step - loss: 0.0427 - val_loss: 0.5950\n",
            "\n",
            "Epoch 131/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0408 - val_loss: 0.5976\n",
            "\n",
            "Epoch 132/1000\n",
            "40/40 [==============================]\n",
            " - 0s 939us/step - loss: 0.0365 - val_loss: 0.6001\n",
            "\n",
            "Epoch 133/1000\n",
            "40/40 [==============================]\n",
            " - 0s 872us/step - loss: 0.0397 - val_loss: 0.6021\n",
            "\n",
            "Epoch 134/1000\n",
            "40/40 [==============================]\n",
            " - 0s 766us/step - loss: 0.0348 - val_loss: 0.6036\n",
            "\n",
            "Epoch 135/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0413 - val_loss: 0.6047\n",
            "\n",
            "Epoch 136/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0443 - val_loss: 0.6049\n",
            "\n",
            "Epoch 137/1000\n",
            "40/40 [==============================]\n",
            " - 0s 677us/step - loss: 0.0434 - val_loss: 0.6050\n",
            "\n",
            "Epoch 138/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0443 - val_loss: 0.6044\n",
            "\n",
            "Epoch 139/1000\n",
            "40/40 [==============================]\n",
            " - 0s 887us/step - loss: 0.0366 - val_loss: 0.6039\n",
            "\n",
            "Epoch 140/1000\n",
            "40/40 [==============================]\n",
            " - 0s 848us/step - loss: 0.0359 - val_loss: 0.6034\n",
            "\n",
            "Epoch 141/1000\n",
            "40/40 [==============================]\n",
            " - 0s 704us/step - loss: 0.0397 - val_loss: 0.6023\n",
            "\n",
            "Epoch 142/1000\n",
            "40/40 [==============================]\n",
            " - 0s 804us/step - loss: 0.0392 - val_loss: 0.6022\n",
            "\n",
            "Epoch 143/1000\n",
            "40/40 [==============================]\n",
            " - 0s 608us/step - loss: 0.0373 - val_loss: 0.6026\n",
            "\n",
            "Epoch 144/1000\n",
            "40/40 [==============================]\n",
            " - 0s 845us/step - loss: 0.0420 - val_loss: 0.6029\n",
            "\n",
            "Epoch 145/1000\n",
            "40/40 [==============================]\n",
            " - 0s 732us/step - loss: 0.0402 - val_loss: 0.6039\n",
            "\n",
            "Epoch 146/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0370 - val_loss: 0.6047\n",
            "\n",
            "Epoch 147/1000\n",
            "40/40 [==============================]\n",
            " - 0s 836us/step - loss: 0.0483 - val_loss: 0.6059\n",
            "\n",
            "Epoch 148/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0434 - val_loss: 0.6081\n",
            "\n",
            "Epoch 149/1000\n",
            "40/40 [==============================]\n",
            " - 0s 931us/step - loss: 0.0422 - val_loss: 0.6105\n",
            "\n",
            "Epoch 150/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0403 - val_loss: 0.6124\n",
            "\n",
            "Epoch 151/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0283 - val_loss: 0.6136\n",
            "\n",
            "Epoch 152/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0338 - val_loss: 0.6143\n",
            "\n",
            "Epoch 153/1000\n",
            "40/40 [==============================]\n",
            " - 0s 764us/step - loss: 0.0460 - val_loss: 0.6149\n",
            "\n",
            "Epoch 154/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0387 - val_loss: 0.6153\n",
            "\n",
            "Epoch 155/1000\n",
            "40/40 [==============================]\n",
            " - 0s 861us/step - loss: 0.0416 - val_loss: 0.6156\n",
            "\n",
            "Epoch 156/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0369 - val_loss: 0.6153\n",
            "\n",
            "Epoch 157/1000\n",
            "40/40 [==============================]\n",
            " - 0s 848us/step - loss: 0.0361 - val_loss: 0.6144\n",
            "\n",
            "Epoch 158/1000\n",
            "40/40 [==============================]\n",
            " - 0s 816us/step - loss: 0.0404 - val_loss: 0.6143\n",
            "\n",
            "Epoch 159/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0365 - val_loss: 0.6150\n",
            "\n",
            "Epoch 160/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0402 - val_loss: 0.6155\n",
            "\n",
            "Epoch 161/1000\n",
            "40/40 [==============================]\n",
            " - 0s 826us/step - loss: 0.0441 - val_loss: 0.6168\n",
            "\n",
            "Epoch 162/1000\n",
            "40/40 [==============================]\n",
            " - 0s 909us/step - loss: 0.0351 - val_loss: 0.6176\n",
            "\n",
            "51/51 [==============================]\n",
            " - 0s 62us/step\n",
            "\n",
            "Window size 1 score = 0.15985724329948425\n",
            "Window size is 7\n",
            "  3%|▎         | 3/100 [01:15<40:06, 24.81s/it, best loss: 0.15985724329948425]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1991-12-24</td>\n",
              "      <td>1.937112</td>\n",
              "      <td>1.929618</td>\n",
              "      <td>1.937112</td>\n",
              "      <td>1.930153</td>\n",
              "      <td>1.937112</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1991-12-10</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.786161</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.787785</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1991-02-19</td>\n",
              "      <td>0.279996</td>\n",
              "      <td>0.279586</td>\n",
              "      <td>0.284947</td>\n",
              "      <td>0.292652</td>\n",
              "      <td>0.279996</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1991-03-05</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.261587</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.251792</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1991-12-03</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>1.698857</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>1.701144</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1991-02-26</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.288209</td>\n",
              "      <td>0.275045</td>\n",
              "      <td>0.280458</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1991-07-09</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.295861</td>\n",
              "      <td>0.282524</td>\n",
              "      <td>0.289122</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1991-04-09</td>\n",
              "      <td>0.115348</td>\n",
              "      <td>0.144751</td>\n",
              "      <td>0.134836</td>\n",
              "      <td>0.139052</td>\n",
              "      <td>0.115348</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1991-06-04</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.107027</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.121403</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "51 1991-12-24  1.937112  1.929618  1.937112  1.930153   1.937112     0.0\n",
              "49 1991-12-10  1.791109  1.786161  1.791109  1.787785   1.791109     0.0\n",
              "7  1991-02-19  0.279996  0.279586  0.284947  0.292652   0.279996     0.0\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "9  1991-03-05  0.226377  0.261587  0.249026  0.251792   0.226377     0.0\n",
              "48 1991-12-03  1.711261  1.698857  1.711261  1.701144   1.711261     0.0\n",
              "8  1991-02-26  0.254609  0.288209  0.275045  0.280458   0.254609     0.0\n",
              "27 1991-07-09  0.266723  0.295861  0.282524  0.289122   0.266723     0.0\n",
              "14 1991-04-09  0.115348  0.144751  0.134836  0.139052   0.115348     0.0\n",
              "22 1991-06-04  0.145897  0.107027  0.145897  0.121403   0.145897     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36 samples, validate on 9 samples\n",
            "Epoch 1/1000\n",
            "36/36 [==============================]\n",
            " - 7s 183ms/step - loss: 0.2997 - val_loss: 1.1416\n",
            "\n",
            "Epoch 2/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2983 - val_loss: 1.1381\n",
            "\n",
            "Epoch 3/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2968 - val_loss: 1.1333\n",
            "\n",
            "Epoch 4/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2954 - val_loss: 1.1267\n",
            "\n",
            "Epoch 5/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2931 - val_loss: 1.1171\n",
            "\n",
            "Epoch 6/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2901 - val_loss: 1.1035\n",
            "\n",
            "Epoch 7/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2864 - val_loss: 1.0841\n",
            "\n",
            "Epoch 8/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2813 - val_loss: 1.0568\n",
            "\n",
            "Epoch 9/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2750 - val_loss: 1.0186\n",
            "\n",
            "Epoch 10/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2656 - val_loss: 0.9668\n",
            "\n",
            "Epoch 11/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2574 - val_loss: 0.8972\n",
            "\n",
            "Epoch 12/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2407 - val_loss: 0.8081\n",
            "\n",
            "Epoch 13/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2223 - val_loss: 0.6996\n",
            "\n",
            "Epoch 14/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1980 - val_loss: 0.6182\n",
            "\n",
            "Epoch 15/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1717 - val_loss: 0.5978\n",
            "\n",
            "Epoch 16/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1610 - val_loss: 0.5978\n",
            "\n",
            "Epoch 17/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1529 - val_loss: 0.5978\n",
            "\n",
            "Epoch 18/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1569 - val_loss: 0.5978\n",
            "\n",
            "Epoch 19/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1528 - val_loss: 0.5978\n",
            "\n",
            "Epoch 20/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1530 - val_loss: 0.5978\n",
            "\n",
            "Epoch 21/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1587 - val_loss: 0.5978\n",
            "\n",
            "Epoch 22/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1598 - val_loss: 0.5978\n",
            "\n",
            "Epoch 23/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1547 - val_loss: 0.5978\n",
            "\n",
            "Epoch 24/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1426 - val_loss: 0.5978\n",
            "\n",
            "Epoch 25/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1469 - val_loss: 0.5978\n",
            "\n",
            "Epoch 26/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1456 - val_loss: 0.5978\n",
            "\n",
            "Epoch 27/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1411 - val_loss: 0.5978\n",
            "\n",
            "Epoch 28/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1421 - val_loss: 0.5978\n",
            "\n",
            "Epoch 29/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1335 - val_loss: 0.5978\n",
            "\n",
            "Epoch 30/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1335 - val_loss: 0.5978\n",
            "\n",
            "Epoch 31/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1324 - val_loss: 0.5978\n",
            "\n",
            "Epoch 32/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1337 - val_loss: 0.5978\n",
            "\n",
            "Epoch 33/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1292 - val_loss: 0.5978\n",
            "\n",
            "Epoch 34/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1350 - val_loss: 0.5978\n",
            "\n",
            "Epoch 35/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1397 - val_loss: 0.5978\n",
            "\n",
            "Epoch 36/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1272 - val_loss: 0.5978\n",
            "\n",
            "Epoch 37/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1369 - val_loss: 0.5978\n",
            "\n",
            "Epoch 38/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1242 - val_loss: 0.5978\n",
            "\n",
            "Epoch 39/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1227 - val_loss: 0.5978\n",
            "\n",
            "Epoch 40/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1268 - val_loss: 0.5978\n",
            "\n",
            "Epoch 41/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1294 - val_loss: 0.5978\n",
            "\n",
            "Epoch 42/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1226 - val_loss: 0.5978\n",
            "\n",
            "Epoch 43/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1275 - val_loss: 0.5978\n",
            "\n",
            "Epoch 44/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1194 - val_loss: 0.5978\n",
            "\n",
            "Epoch 45/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1224 - val_loss: 0.5978\n",
            "\n",
            "Epoch 46/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1237 - val_loss: 0.5978\n",
            "\n",
            "Epoch 47/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1357 - val_loss: 0.5978\n",
            "\n",
            "Epoch 48/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1279 - val_loss: 0.5978\n",
            "\n",
            "Epoch 49/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1309 - val_loss: 0.5978\n",
            "\n",
            "Epoch 50/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1158 - val_loss: 0.5978\n",
            "\n",
            "Epoch 51/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1283 - val_loss: 0.5978\n",
            "\n",
            "Epoch 52/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1305 - val_loss: 0.5978\n",
            "\n",
            "Epoch 53/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1241 - val_loss: 0.5978\n",
            "\n",
            "Epoch 54/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5978\n",
            "\n",
            "Epoch 55/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1197 - val_loss: 0.5978\n",
            "\n",
            "Epoch 56/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1322 - val_loss: 0.5978\n",
            "\n",
            "Epoch 57/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1171 - val_loss: 0.5978\n",
            "\n",
            "Epoch 58/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1256 - val_loss: 0.6005\n",
            "\n",
            "Epoch 59/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1260 - val_loss: 0.6260\n",
            "\n",
            "Epoch 60/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1221 - val_loss: 0.6576\n",
            "\n",
            "Epoch 61/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1171 - val_loss: 0.6939\n",
            "\n",
            "Epoch 62/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1219 - val_loss: 0.7239\n",
            "\n",
            "Epoch 63/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1259 - val_loss: 0.7435\n",
            "\n",
            "Epoch 64/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1283 - val_loss: 0.7607\n",
            "\n",
            "Epoch 65/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1158 - val_loss: 0.7846\n",
            "\n",
            "Epoch 66/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1142 - val_loss: 0.8011\n",
            "\n",
            "Epoch 67/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1137 - val_loss: 0.8021\n",
            "\n",
            "Epoch 68/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1177 - val_loss: 0.7911\n",
            "\n",
            "Epoch 69/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1157 - val_loss: 0.7664\n",
            "\n",
            "Epoch 70/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1213 - val_loss: 0.7398\n",
            "\n",
            "Epoch 71/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1049 - val_loss: 0.7170\n",
            "\n",
            "Epoch 72/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1117 - val_loss: 0.6972\n",
            "\n",
            "Epoch 73/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1135 - val_loss: 0.6791\n",
            "\n",
            "Epoch 74/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1186 - val_loss: 0.6714\n",
            "\n",
            "Epoch 75/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1133 - val_loss: 0.6708\n",
            "\n",
            "Epoch 76/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1151 - val_loss: 0.6786\n",
            "\n",
            "Epoch 77/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1147 - val_loss: 0.6838\n",
            "\n",
            "Epoch 78/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1119 - val_loss: 0.6924\n",
            "\n",
            "Epoch 79/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1117 - val_loss: 0.7051\n",
            "\n",
            "Epoch 80/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1046 - val_loss: 0.7141\n",
            "\n",
            "Epoch 81/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1089 - val_loss: 0.7224\n",
            "\n",
            "Epoch 82/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1070 - val_loss: 0.7293\n",
            "\n",
            "Epoch 83/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1208 - val_loss: 0.7283\n",
            "\n",
            "Epoch 84/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1116 - val_loss: 0.7197\n",
            "\n",
            "Epoch 85/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1071 - val_loss: 0.7082\n",
            "\n",
            "Epoch 86/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1138 - val_loss: 0.6988\n",
            "\n",
            "Epoch 87/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1057 - val_loss: 0.6881\n",
            "\n",
            "Epoch 88/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1079 - val_loss: 0.6751\n",
            "\n",
            "Epoch 89/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1075 - val_loss: 0.6641\n",
            "\n",
            "Epoch 90/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1061 - val_loss: 0.6568\n",
            "\n",
            "Epoch 91/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1071 - val_loss: 0.6552\n",
            "\n",
            "Epoch 92/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1043 - val_loss: 0.6577\n",
            "\n",
            "Epoch 93/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1113 - val_loss: 0.6613\n",
            "\n",
            "Epoch 94/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1045 - val_loss: 0.6660\n",
            "\n",
            "Epoch 95/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1076 - val_loss: 0.6716\n",
            "\n",
            "Epoch 96/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1054 - val_loss: 0.6735\n",
            "\n",
            "Epoch 97/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1040 - val_loss: 0.6746\n",
            "\n",
            "Epoch 98/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1112 - val_loss: 0.6788\n",
            "\n",
            "Epoch 99/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1140 - val_loss: 0.6799\n",
            "\n",
            "Epoch 100/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1054 - val_loss: 0.6832\n",
            "\n",
            "Epoch 101/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1080 - val_loss: 0.6920\n",
            "\n",
            "Epoch 102/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0986 - val_loss: 0.7032\n",
            "\n",
            "Epoch 103/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1126 - val_loss: 0.7082\n",
            "\n",
            "Epoch 104/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1003 - val_loss: 0.7147\n",
            "\n",
            "Epoch 105/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1086 - val_loss: 0.7177\n",
            "\n",
            "Epoch 106/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1067 - val_loss: 0.7205\n",
            "\n",
            "Epoch 107/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1090 - val_loss: 0.7195\n",
            "\n",
            "Epoch 108/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1064 - val_loss: 0.7123\n",
            "\n",
            "Epoch 109/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1050 - val_loss: 0.7015\n",
            "\n",
            "Epoch 110/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1005 - val_loss: 0.6901\n",
            "\n",
            "Epoch 111/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1062 - val_loss: 0.6778\n",
            "\n",
            "Epoch 112/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1067 - val_loss: 0.6678\n",
            "\n",
            "Epoch 113/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1109 - val_loss: 0.6554\n",
            "\n",
            "Epoch 114/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.6478\n",
            "\n",
            "Epoch 115/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1060 - val_loss: 0.6394\n",
            "\n",
            "45/45 [==============================]\n",
            " - 0s 277us/step\n",
            "\n",
            "Window size 7 score = 0.20974840223789215\n",
            "Window size is 5\n",
            "  4%|▍         | 4/100 [01:46<42:32, 26.58s/it, best loss: 0.15985724329948425]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1991-04-02</td>\n",
              "      <td>0.139787</td>\n",
              "      <td>0.151110</td>\n",
              "      <td>0.146424</td>\n",
              "      <td>0.163868</td>\n",
              "      <td>0.139787</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1991-05-28</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.048394</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.063215</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1991-10-15</td>\n",
              "      <td>1.018751</td>\n",
              "      <td>0.963785</td>\n",
              "      <td>1.018751</td>\n",
              "      <td>0.971655</td>\n",
              "      <td>1.018751</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1991-02-19</td>\n",
              "      <td>0.279996</td>\n",
              "      <td>0.279586</td>\n",
              "      <td>0.284947</td>\n",
              "      <td>0.292652</td>\n",
              "      <td>0.279996</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1991-01-01</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1991-07-23</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.347058</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.359611</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1991-04-30</td>\n",
              "      <td>0.039924</td>\n",
              "      <td>0.072753</td>\n",
              "      <td>0.064469</td>\n",
              "      <td>0.062467</td>\n",
              "      <td>0.039924</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1991-12-10</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.786161</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.787785</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1991-07-16</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.294029</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.306985</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "13 1991-04-02  0.139787  0.151110  0.146424  0.163868   0.139787     0.0\n",
              "21 1991-05-28  0.085853  0.048394  0.085853  0.063215   0.085853     0.0\n",
              "41 1991-10-15  1.018751  0.963785  1.018751  0.971655   1.018751     0.0\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "7  1991-02-19  0.279996  0.279586  0.284947  0.292652   0.279996     0.0\n",
              "0  1991-01-01  0.255346  0.233348  0.255346  0.246764   0.255346     0.0\n",
              "29 1991-07-23  0.354998  0.347058  0.354998  0.359611   0.354998     0.0\n",
              "17 1991-04-30  0.039924  0.072753  0.064469  0.062467   0.039924     0.0\n",
              "49 1991-12-10  1.791109  1.786161  1.791109  1.787785   1.791109     0.0\n",
              "28 1991-07-16  0.318656  0.294029  0.318656  0.306985   0.318656     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 37 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "37/37 [==============================]\n",
            " - 7s 199ms/step - loss: 0.2951 - val_loss: 1.0729\n",
            "\n",
            "Epoch 2/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2938 - val_loss: 1.0732\n",
            "\n",
            "Epoch 3/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2925 - val_loss: 1.0733\n",
            "\n",
            "Epoch 4/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2911 - val_loss: 1.0733\n",
            "\n",
            "Epoch 5/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2893 - val_loss: 1.0731\n",
            "\n",
            "Epoch 6/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2872 - val_loss: 1.0721\n",
            "\n",
            "Epoch 7/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2852 - val_loss: 1.0699\n",
            "\n",
            "Epoch 8/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2817 - val_loss: 1.0661\n",
            "\n",
            "Epoch 9/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2783 - val_loss: 1.0600\n",
            "\n",
            "Epoch 10/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2735 - val_loss: 1.0506\n",
            "\n",
            "Epoch 11/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2669 - val_loss: 1.0367\n",
            "\n",
            "Epoch 12/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2607 - val_loss: 1.0169\n",
            "\n",
            "Epoch 13/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2509 - val_loss: 0.9894\n",
            "\n",
            "Epoch 14/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2418 - val_loss: 0.9527\n",
            "\n",
            "Epoch 15/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2272 - val_loss: 0.9061\n",
            "\n",
            "Epoch 16/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2128 - val_loss: 0.8488\n",
            "\n",
            "Epoch 17/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1897 - val_loss: 0.7824\n",
            "\n",
            "Epoch 18/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1715 - val_loss: 0.7102\n",
            "\n",
            "Epoch 19/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1455 - val_loss: 0.6370\n",
            "\n",
            "Epoch 20/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1294 - val_loss: 0.5839\n",
            "\n",
            "Epoch 21/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1267 - val_loss: 0.5572\n",
            "\n",
            "Epoch 22/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1099 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1046 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1037 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1037 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1072 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1053 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1102 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1121 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1108 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1306 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1186 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1262 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1237 - val_loss: 0.5642\n",
            "\n",
            "Epoch 35/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1164 - val_loss: 0.5958\n",
            "\n",
            "Epoch 36/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1189 - val_loss: 0.6278\n",
            "\n",
            "Epoch 37/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1101 - val_loss: 0.6588\n",
            "\n",
            "Epoch 38/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1113 - val_loss: 0.6877\n",
            "\n",
            "Epoch 39/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0997 - val_loss: 0.7130\n",
            "\n",
            "Epoch 40/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1065 - val_loss: 0.7391\n",
            "\n",
            "Epoch 41/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1027 - val_loss: 0.7649\n",
            "\n",
            "Epoch 42/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0973 - val_loss: 0.7816\n",
            "\n",
            "Epoch 43/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1005 - val_loss: 0.7885\n",
            "\n",
            "Epoch 44/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0995 - val_loss: 0.7829\n",
            "\n",
            "Epoch 45/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1064 - val_loss: 0.7694\n",
            "\n",
            "Epoch 46/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0978 - val_loss: 0.7526\n",
            "\n",
            "Epoch 47/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1023 - val_loss: 0.7349\n",
            "\n",
            "Epoch 48/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1043 - val_loss: 0.7172\n",
            "\n",
            "Epoch 49/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1001 - val_loss: 0.7013\n",
            "\n",
            "Epoch 50/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0954 - val_loss: 0.6833\n",
            "\n",
            "Epoch 51/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0915 - val_loss: 0.6663\n",
            "\n",
            "Epoch 52/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0994 - val_loss: 0.6535\n",
            "\n",
            "Epoch 53/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1033 - val_loss: 0.6412\n",
            "\n",
            "Epoch 54/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1013 - val_loss: 0.6315\n",
            "\n",
            "Epoch 55/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1043 - val_loss: 0.6272\n",
            "\n",
            "Epoch 56/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0985 - val_loss: 0.6276\n",
            "\n",
            "Epoch 57/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.6271\n",
            "\n",
            "Epoch 58/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0994 - val_loss: 0.6285\n",
            "\n",
            "Epoch 59/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1017 - val_loss: 0.6322\n",
            "\n",
            "Epoch 60/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0964 - val_loss: 0.6360\n",
            "\n",
            "Epoch 61/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0918 - val_loss: 0.6422\n",
            "\n",
            "Epoch 62/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0962 - val_loss: 0.6514\n",
            "\n",
            "Epoch 63/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1018 - val_loss: 0.6659\n",
            "\n",
            "Epoch 64/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0968 - val_loss: 0.6779\n",
            "\n",
            "Epoch 65/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0937 - val_loss: 0.6878\n",
            "\n",
            "Epoch 66/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0928 - val_loss: 0.6922\n",
            "\n",
            "Epoch 67/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0968 - val_loss: 0.6973\n",
            "\n",
            "Epoch 68/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0937 - val_loss: 0.7003\n",
            "\n",
            "Epoch 69/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0967 - val_loss: 0.7050\n",
            "\n",
            "Epoch 70/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0947 - val_loss: 0.7089\n",
            "\n",
            "Epoch 71/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0874 - val_loss: 0.7140\n",
            "\n",
            "Epoch 72/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0907 - val_loss: 0.7191\n",
            "\n",
            "Epoch 73/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0932 - val_loss: 0.7223\n",
            "\n",
            "Epoch 74/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0948 - val_loss: 0.7233\n",
            "\n",
            "Epoch 75/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0918 - val_loss: 0.7204\n",
            "\n",
            "Epoch 76/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0911 - val_loss: 0.7187\n",
            "\n",
            "Epoch 77/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0846 - val_loss: 0.7132\n",
            "\n",
            "Epoch 78/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0959 - val_loss: 0.7125\n",
            "\n",
            "Epoch 79/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0955 - val_loss: 0.7151\n",
            "\n",
            "Epoch 80/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0979 - val_loss: 0.7128\n",
            "\n",
            "Epoch 81/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.7127\n",
            "\n",
            "Epoch 82/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0898 - val_loss: 0.7155\n",
            "\n",
            "Epoch 83/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0952 - val_loss: 0.7149\n",
            "\n",
            "Epoch 84/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0932 - val_loss: 0.7167\n",
            "\n",
            "Epoch 85/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0907 - val_loss: 0.7203\n",
            "\n",
            "Epoch 86/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0899 - val_loss: 0.7261\n",
            "\n",
            "Epoch 87/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0932 - val_loss: 0.7306\n",
            "\n",
            "Epoch 88/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0902 - val_loss: 0.7311\n",
            "\n",
            "Epoch 89/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0903 - val_loss: 0.7325\n",
            "\n",
            "Epoch 90/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0902 - val_loss: 0.7356\n",
            "\n",
            "Epoch 91/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0935 - val_loss: 0.7411\n",
            "\n",
            "Epoch 92/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0878 - val_loss: 0.7399\n",
            "\n",
            "Epoch 93/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0926 - val_loss: 0.7379\n",
            "\n",
            "Epoch 94/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0880 - val_loss: 0.7301\n",
            "\n",
            "Epoch 95/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0843 - val_loss: 0.7161\n",
            "\n",
            "Epoch 96/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0906 - val_loss: 0.7053\n",
            "\n",
            "Epoch 97/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0893 - val_loss: 0.6993\n",
            "\n",
            "Epoch 98/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0916 - val_loss: 0.6929\n",
            "\n",
            "Epoch 99/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0890 - val_loss: 0.6812\n",
            "\n",
            "Epoch 100/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0873 - val_loss: 0.6757\n",
            "\n",
            "Epoch 101/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0878 - val_loss: 0.6729\n",
            "\n",
            "Epoch 102/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0977 - val_loss: 0.6746\n",
            "\n",
            "Epoch 103/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1002 - val_loss: 0.6803\n",
            "\n",
            "Epoch 104/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0921 - val_loss: 0.6877\n",
            "\n",
            "Epoch 105/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0905 - val_loss: 0.6971\n",
            "\n",
            "Epoch 106/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0847 - val_loss: 0.7074\n",
            "\n",
            "Epoch 107/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0879 - val_loss: 0.7119\n",
            "\n",
            "Epoch 108/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0912 - val_loss: 0.7152\n",
            "\n",
            "Epoch 109/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0940 - val_loss: 0.7202\n",
            "\n",
            "Epoch 110/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0891 - val_loss: 0.7238\n",
            "\n",
            "Epoch 111/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0880 - val_loss: 0.7294\n",
            "\n",
            "Epoch 112/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0899 - val_loss: 0.7333\n",
            "\n",
            "Epoch 113/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0886 - val_loss: 0.7372\n",
            "\n",
            "Epoch 114/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0858 - val_loss: 0.7411\n",
            "\n",
            "Epoch 115/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0854 - val_loss: 0.7388\n",
            "\n",
            "Epoch 116/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0858 - val_loss: 0.7245\n",
            "\n",
            "Epoch 117/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0855 - val_loss: 0.7126\n",
            "\n",
            "Epoch 118/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0826 - val_loss: 0.7033\n",
            "\n",
            "Epoch 119/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0794 - val_loss: 0.6926\n",
            "\n",
            "Epoch 120/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0851 - val_loss: 0.6825\n",
            "\n",
            "Epoch 121/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0785 - val_loss: 0.6711\n",
            "\n",
            "Epoch 122/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0853 - val_loss: 0.6660\n",
            "\n",
            "47/47 [==============================]\n",
            " - 0s 220us/step\n",
            "\n",
            "Window size 5 score = 0.2060663104057312\n",
            "Window size is 1\n",
            "  5%|▌         | 5/100 [02:19<44:57, 28.39s/it, best loss: 0.15985724329948425]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1991-07-30</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.379392</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.391700</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1991-07-09</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.295861</td>\n",
              "      <td>0.282524</td>\n",
              "      <td>0.289122</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1991-02-05</td>\n",
              "      <td>0.243864</td>\n",
              "      <td>0.230545</td>\n",
              "      <td>0.243864</td>\n",
              "      <td>0.243983</td>\n",
              "      <td>0.243864</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1991-10-08</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.865488</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.874104</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1991-09-10</td>\n",
              "      <td>0.862109</td>\n",
              "      <td>0.874542</td>\n",
              "      <td>0.862109</td>\n",
              "      <td>0.860520</td>\n",
              "      <td>0.862109</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1991-06-18</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.213839</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.227404</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1991-06-11</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.169217</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.183121</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1991-09-03</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.805993</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.815060</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1991-07-23</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.347058</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.359611</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "30 1991-07-30  0.424839  0.379392  0.424839  0.391700   0.424839     0.0\n",
              "27 1991-07-09  0.266723  0.295861  0.282524  0.289122   0.266723     0.0\n",
              "5  1991-02-05  0.243864  0.230545  0.243864  0.243983   0.243864     0.0\n",
              "40 1991-10-08  0.914885  0.865488  0.914885  0.874104   0.914885     0.0\n",
              "19 1991-05-14  0.000000  0.000000  0.000000  0.000000   0.000000     0.0\n",
              "36 1991-09-10  0.862109  0.874542  0.862109  0.860520   0.862109     0.0\n",
              "24 1991-06-18  0.256505  0.213839  0.256505  0.227404   0.256505     0.0\n",
              "23 1991-06-11  0.189087  0.169217  0.189087  0.183121   0.189087     0.0\n",
              "35 1991-09-03  0.857685  0.805993  0.857685  0.815060   0.857685     0.0\n",
              "29 1991-07-23  0.354998  0.347058  0.354998  0.359611   0.354998     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 40 samples, validate on 11 samples\n",
            "Epoch 1/1000\n",
            "40/40 [==============================]\n",
            " - 8s 202ms/step - loss: 0.2917 - val_loss: 1.0025\n",
            "\n",
            "Epoch 2/1000\n",
            "40/40 [==============================]\n",
            " - 0s 877us/step - loss: 0.2913 - val_loss: 1.0034\n",
            "\n",
            "Epoch 3/1000\n",
            "40/40 [==============================]\n",
            " - 0s 904us/step - loss: 0.2908 - val_loss: 1.0043\n",
            "\n",
            "Epoch 4/1000\n",
            "40/40 [==============================]\n",
            " - 0s 697us/step - loss: 0.2903 - val_loss: 1.0053\n",
            "\n",
            "Epoch 5/1000\n",
            "40/40 [==============================]\n",
            " - 0s 831us/step - loss: 0.2898 - val_loss: 1.0063\n",
            "\n",
            "Epoch 6/1000\n",
            "40/40 [==============================]\n",
            " - 0s 766us/step - loss: 0.2894 - val_loss: 1.0073\n",
            "\n",
            "Epoch 7/1000\n",
            "40/40 [==============================]\n",
            " - 0s 861us/step - loss: 0.2888 - val_loss: 1.0083\n",
            "\n",
            "Epoch 8/1000\n",
            "40/40 [==============================]\n",
            " - 0s 839us/step - loss: 0.2883 - val_loss: 1.0094\n",
            "\n",
            "Epoch 9/1000\n",
            "40/40 [==============================]\n",
            " - 0s 775us/step - loss: 0.2877 - val_loss: 1.0106\n",
            "\n",
            "Epoch 10/1000\n",
            "40/40 [==============================]\n",
            " - 0s 779us/step - loss: 0.2871 - val_loss: 1.0118\n",
            "\n",
            "Epoch 11/1000\n",
            "40/40 [==============================]\n",
            " - 0s 725us/step - loss: 0.2864 - val_loss: 1.0130\n",
            "\n",
            "Epoch 12/1000\n",
            "40/40 [==============================]\n",
            " - 0s 864us/step - loss: 0.2857 - val_loss: 1.0143\n",
            "\n",
            "Epoch 13/1000\n",
            "40/40 [==============================]\n",
            " - 0s 815us/step - loss: 0.2852 - val_loss: 1.0156\n",
            "\n",
            "Epoch 14/1000\n",
            "40/40 [==============================]\n",
            " - 0s 979us/step - loss: 0.2843 - val_loss: 1.0170\n",
            "\n",
            "Epoch 15/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2836 - val_loss: 1.0185\n",
            "\n",
            "Epoch 16/1000\n",
            "40/40 [==============================]\n",
            " - 0s 721us/step - loss: 0.2826 - val_loss: 1.0199\n",
            "\n",
            "Epoch 17/1000\n",
            "40/40 [==============================]\n",
            " - 0s 875us/step - loss: 0.2820 - val_loss: 1.0215\n",
            "\n",
            "Epoch 18/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2805 - val_loss: 1.0230\n",
            "\n",
            "Epoch 19/1000\n",
            "40/40 [==============================]\n",
            " - 0s 702us/step - loss: 0.2797 - val_loss: 1.0247\n",
            "\n",
            "Epoch 20/1000\n",
            "40/40 [==============================]\n",
            " - 0s 984us/step - loss: 0.2783 - val_loss: 1.0263\n",
            "\n",
            "Epoch 21/1000\n",
            "40/40 [==============================]\n",
            " - 0s 684us/step - loss: 0.2772 - val_loss: 1.0280\n",
            "\n",
            "Epoch 22/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2759 - val_loss: 1.0297\n",
            "\n",
            "Epoch 23/1000\n",
            "40/40 [==============================]\n",
            " - 0s 928us/step - loss: 0.2740 - val_loss: 1.0314\n",
            "\n",
            "Epoch 24/1000\n",
            "40/40 [==============================]\n",
            " - 0s 911us/step - loss: 0.2732 - val_loss: 1.0332\n",
            "\n",
            "Epoch 25/1000\n",
            "40/40 [==============================]\n",
            " - 0s 743us/step - loss: 0.2716 - val_loss: 1.0348\n",
            "\n",
            "Epoch 26/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2700 - val_loss: 1.0364\n",
            "\n",
            "Epoch 27/1000\n",
            "40/40 [==============================]\n",
            " - 0s 673us/step - loss: 0.2679 - val_loss: 1.0379\n",
            "\n",
            "Epoch 28/1000\n",
            "40/40 [==============================]\n",
            " - 0s 933us/step - loss: 0.2655 - val_loss: 1.0392\n",
            "\n",
            "Epoch 29/1000\n",
            "40/40 [==============================]\n",
            " - 0s 912us/step - loss: 0.2632 - val_loss: 1.0404\n",
            "\n",
            "Epoch 30/1000\n",
            "40/40 [==============================]\n",
            " - 0s 764us/step - loss: 0.2608 - val_loss: 1.0412\n",
            "\n",
            "Epoch 31/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2571 - val_loss: 1.0418\n",
            "\n",
            "Epoch 32/1000\n",
            "40/40 [==============================]\n",
            " - 0s 888us/step - loss: 0.2547 - val_loss: 1.0420\n",
            "\n",
            "Epoch 33/1000\n",
            "40/40 [==============================]\n",
            " - 0s 740us/step - loss: 0.2515 - val_loss: 1.0417\n",
            "\n",
            "Epoch 34/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2495 - val_loss: 1.0406\n",
            "\n",
            "Epoch 35/1000\n",
            "40/40 [==============================]\n",
            " - 0s 956us/step - loss: 0.2439 - val_loss: 1.0387\n",
            "\n",
            "Epoch 36/1000\n",
            "40/40 [==============================]\n",
            " - 0s 847us/step - loss: 0.2403 - val_loss: 1.0356\n",
            "\n",
            "Epoch 37/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2348 - val_loss: 1.0313\n",
            "\n",
            "Epoch 38/1000\n",
            "40/40 [==============================]\n",
            " - 0s 933us/step - loss: 0.2303 - val_loss: 1.0254\n",
            "\n",
            "Epoch 39/1000\n",
            "40/40 [==============================]\n",
            " - 0s 820us/step - loss: 0.2241 - val_loss: 1.0180\n",
            "\n",
            "Epoch 40/1000\n",
            "40/40 [==============================]\n",
            " - 0s 769us/step - loss: 0.2183 - val_loss: 1.0087\n",
            "\n",
            "Epoch 41/1000\n",
            "40/40 [==============================]\n",
            " - 0s 804us/step - loss: 0.2121 - val_loss: 0.9974\n",
            "\n",
            "Epoch 42/1000\n",
            "40/40 [==============================]\n",
            " - 0s 740us/step - loss: 0.2045 - val_loss: 0.9839\n",
            "\n",
            "Epoch 43/1000\n",
            "40/40 [==============================]\n",
            " - 0s 690us/step - loss: 0.1952 - val_loss: 0.9682\n",
            "\n",
            "Epoch 44/1000\n",
            "40/40 [==============================]\n",
            " - 0s 765us/step - loss: 0.1847 - val_loss: 0.9499\n",
            "\n",
            "Epoch 45/1000\n",
            "40/40 [==============================]\n",
            " - 0s 791us/step - loss: 0.1772 - val_loss: 0.9291\n",
            "\n",
            "Epoch 46/1000\n",
            "40/40 [==============================]\n",
            " - 0s 623us/step - loss: 0.1637 - val_loss: 0.9059\n",
            "\n",
            "Epoch 47/1000\n",
            "40/40 [==============================]\n",
            " - 0s 691us/step - loss: 0.1558 - val_loss: 0.8801\n",
            "\n",
            "Epoch 48/1000\n",
            "40/40 [==============================]\n",
            " - 0s 706us/step - loss: 0.1392 - val_loss: 0.8524\n",
            "\n",
            "Epoch 49/1000\n",
            "40/40 [==============================]\n",
            " - 0s 698us/step - loss: 0.1301 - val_loss: 0.8229\n",
            "\n",
            "Epoch 50/1000\n",
            "40/40 [==============================]\n",
            " - 0s 604us/step - loss: 0.1224 - val_loss: 0.7923\n",
            "\n",
            "Epoch 51/1000\n",
            "40/40 [==============================]\n",
            " - 0s 684us/step - loss: 0.1242 - val_loss: 0.7614\n",
            "\n",
            "Epoch 52/1000\n",
            "40/40 [==============================]\n",
            " - 0s 666us/step - loss: 0.1123 - val_loss: 0.7312\n",
            "\n",
            "Epoch 53/1000\n",
            "40/40 [==============================]\n",
            " - 0s 640us/step - loss: 0.1045 - val_loss: 0.7030\n",
            "\n",
            "Epoch 54/1000\n",
            "40/40 [==============================]\n",
            " - 0s 767us/step - loss: 0.0952 - val_loss: 0.6771\n",
            "\n",
            "Epoch 55/1000\n",
            "40/40 [==============================]\n",
            " - 0s 636us/step - loss: 0.0878 - val_loss: 0.6536\n",
            "\n",
            "Epoch 56/1000\n",
            "40/40 [==============================]\n",
            " - 0s 635us/step - loss: 0.0709 - val_loss: 0.6325\n",
            "\n",
            "Epoch 57/1000\n",
            "40/40 [==============================]\n",
            " - 0s 742us/step - loss: 0.0618 - val_loss: 0.6139\n",
            "\n",
            "Epoch 58/1000\n",
            "40/40 [==============================]\n",
            " - 0s 736us/step - loss: 0.0516 - val_loss: 0.5965\n",
            "\n",
            "Epoch 59/1000\n",
            "40/40 [==============================]\n",
            " - 0s 670us/step - loss: 0.0445 - val_loss: 0.5806\n",
            "\n",
            "Epoch 60/1000\n",
            "40/40 [==============================]\n",
            " - 0s 712us/step - loss: 0.0442 - val_loss: 0.5667\n",
            "\n",
            "Epoch 61/1000\n",
            "40/40 [==============================]\n",
            " - 0s 674us/step - loss: 0.0498 - val_loss: 0.5574\n",
            "\n",
            "Epoch 62/1000\n",
            "40/40 [==============================]\n",
            " - 0s 769us/step - loss: 0.0555 - val_loss: 0.5520\n",
            "\n",
            "Epoch 63/1000\n",
            "40/40 [==============================]\n",
            " - 0s 616us/step - loss: 0.0621 - val_loss: 0.5503\n",
            "\n",
            "Epoch 64/1000\n",
            "40/40 [==============================]\n",
            " - 0s 681us/step - loss: 0.0537 - val_loss: 0.5515\n",
            "\n",
            "Epoch 65/1000\n",
            "40/40 [==============================]\n",
            " - 0s 764us/step - loss: 0.0538 - val_loss: 0.5551\n",
            "\n",
            "Epoch 66/1000\n",
            "40/40 [==============================]\n",
            " - 0s 793us/step - loss: 0.0529 - val_loss: 0.5602\n",
            "\n",
            "Epoch 67/1000\n",
            "40/40 [==============================]\n",
            " - 0s 848us/step - loss: 0.0498 - val_loss: 0.5655\n",
            "\n",
            "Epoch 68/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0445 - val_loss: 0.5716\n",
            "\n",
            "Epoch 69/1000\n",
            "40/40 [==============================]\n",
            " - 0s 831us/step - loss: 0.0450 - val_loss: 0.5793\n",
            "\n",
            "Epoch 70/1000\n",
            "40/40 [==============================]\n",
            " - 0s 806us/step - loss: 0.0487 - val_loss: 0.5859\n",
            "\n",
            "Epoch 71/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0515 - val_loss: 0.5908\n",
            "\n",
            "Epoch 72/1000\n",
            "40/40 [==============================]\n",
            " - 0s 904us/step - loss: 0.0506 - val_loss: 0.5948\n",
            "\n",
            "Epoch 73/1000\n",
            "40/40 [==============================]\n",
            " - 0s 949us/step - loss: 0.0439 - val_loss: 0.5975\n",
            "\n",
            "Epoch 74/1000\n",
            "40/40 [==============================]\n",
            " - 0s 758us/step - loss: 0.0456 - val_loss: 0.5989\n",
            "\n",
            "Epoch 75/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0453 - val_loss: 0.5998\n",
            "\n",
            "Epoch 76/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0385 - val_loss: 0.6001\n",
            "\n",
            "Epoch 77/1000\n",
            "40/40 [==============================]\n",
            " - 0s 721us/step - loss: 0.0452 - val_loss: 0.6002\n",
            "\n",
            "Epoch 78/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0406 - val_loss: 0.6016\n",
            "\n",
            "Epoch 79/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0420 - val_loss: 0.6028\n",
            "\n",
            "Epoch 80/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0417 - val_loss: 0.6034\n",
            "\n",
            "Epoch 81/1000\n",
            "40/40 [==============================]\n",
            " - 0s 738us/step - loss: 0.0430 - val_loss: 0.6035\n",
            "\n",
            "Epoch 82/1000\n",
            "40/40 [==============================]\n",
            " - 0s 669us/step - loss: 0.0446 - val_loss: 0.6033\n",
            "\n",
            "Epoch 83/1000\n",
            "40/40 [==============================]\n",
            " - 0s 889us/step - loss: 0.0428 - val_loss: 0.6025\n",
            "\n",
            "Epoch 84/1000\n",
            "40/40 [==============================]\n",
            " - 0s 759us/step - loss: 0.0443 - val_loss: 0.6005\n",
            "\n",
            "Epoch 85/1000\n",
            "40/40 [==============================]\n",
            " - 0s 909us/step - loss: 0.0429 - val_loss: 0.5972\n",
            "\n",
            "Epoch 86/1000\n",
            "40/40 [==============================]\n",
            " - 0s 768us/step - loss: 0.0416 - val_loss: 0.5937\n",
            "\n",
            "Epoch 87/1000\n",
            "40/40 [==============================]\n",
            " - 0s 914us/step - loss: 0.0420 - val_loss: 0.5903\n",
            "\n",
            "Epoch 88/1000\n",
            "40/40 [==============================]\n",
            " - 0s 911us/step - loss: 0.0455 - val_loss: 0.5862\n",
            "\n",
            "Epoch 89/1000\n",
            "40/40 [==============================]\n",
            " - 0s 801us/step - loss: 0.0473 - val_loss: 0.5814\n",
            "\n",
            "Epoch 90/1000\n",
            "40/40 [==============================]\n",
            " - 0s 986us/step - loss: 0.0383 - val_loss: 0.5765\n",
            "\n",
            "Epoch 91/1000\n",
            "40/40 [==============================]\n",
            " - 0s 811us/step - loss: 0.0409 - val_loss: 0.5721\n",
            "\n",
            "Epoch 92/1000\n",
            "40/40 [==============================]\n",
            " - 0s 890us/step - loss: 0.0448 - val_loss: 0.5676\n",
            "\n",
            "Epoch 93/1000\n",
            "40/40 [==============================]\n",
            " - 0s 836us/step - loss: 0.0422 - val_loss: 0.5634\n",
            "\n",
            "Epoch 94/1000\n",
            "40/40 [==============================]\n",
            " - 0s 939us/step - loss: 0.0404 - val_loss: 0.5606\n",
            "\n",
            "Epoch 95/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0444 - val_loss: 0.5585\n",
            "\n",
            "Epoch 96/1000\n",
            "40/40 [==============================]\n",
            " - 0s 916us/step - loss: 0.0349 - val_loss: 0.5560\n",
            "\n",
            "Epoch 97/1000\n",
            "40/40 [==============================]\n",
            " - 0s 901us/step - loss: 0.0406 - val_loss: 0.5547\n",
            "\n",
            "Epoch 98/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1000us/step - loss: 0.0438 - val_loss: 0.5551\n",
            "\n",
            "Epoch 99/1000\n",
            "40/40 [==============================]\n",
            " - 0s 890us/step - loss: 0.0385 - val_loss: 0.5560\n",
            "\n",
            "Epoch 100/1000\n",
            "40/40 [==============================]\n",
            " - 0s 711us/step - loss: 0.0405 - val_loss: 0.5585\n",
            "\n",
            "Epoch 101/1000\n",
            "40/40 [==============================]\n",
            " - 0s 698us/step - loss: 0.0390 - val_loss: 0.5622\n",
            "\n",
            "Epoch 102/1000\n",
            "40/40 [==============================]\n",
            " - 0s 736us/step - loss: 0.0409 - val_loss: 0.5656\n",
            "\n",
            "Epoch 103/1000\n",
            "40/40 [==============================]\n",
            " - 0s 892us/step - loss: 0.0393 - val_loss: 0.5683\n",
            "\n",
            "Epoch 104/1000\n",
            "40/40 [==============================]\n",
            " - 0s 740us/step - loss: 0.0368 - val_loss: 0.5719\n",
            "\n",
            "Epoch 105/1000\n",
            "40/40 [==============================]\n",
            " - 0s 952us/step - loss: 0.0441 - val_loss: 0.5747\n",
            "\n",
            "Epoch 106/1000\n",
            "40/40 [==============================]\n",
            " - 0s 769us/step - loss: 0.0389 - val_loss: 0.5770\n",
            "\n",
            "Epoch 107/1000\n",
            "40/40 [==============================]\n",
            " - 0s 774us/step - loss: 0.0390 - val_loss: 0.5793\n",
            "\n",
            "Epoch 108/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0388 - val_loss: 0.5813\n",
            "\n",
            "Epoch 109/1000\n",
            "40/40 [==============================]\n",
            " - 0s 808us/step - loss: 0.0441 - val_loss: 0.5827\n",
            "\n",
            "Epoch 110/1000\n",
            "40/40 [==============================]\n",
            " - 0s 717us/step - loss: 0.0351 - val_loss: 0.5822\n",
            "\n",
            "Epoch 111/1000\n",
            "40/40 [==============================]\n",
            " - 0s 600us/step - loss: 0.0393 - val_loss: 0.5817\n",
            "\n",
            "Epoch 112/1000\n",
            "40/40 [==============================]\n",
            " - 0s 843us/step - loss: 0.0430 - val_loss: 0.5809\n",
            "\n",
            "Epoch 113/1000\n",
            "40/40 [==============================]\n",
            " - 0s 861us/step - loss: 0.0429 - val_loss: 0.5802\n",
            "\n",
            "Epoch 114/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0408 - val_loss: 0.5795\n",
            "\n",
            "Epoch 115/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0410 - val_loss: 0.5775\n",
            "\n",
            "Epoch 116/1000\n",
            "40/40 [==============================]\n",
            " - 0s 749us/step - loss: 0.0411 - val_loss: 0.5769\n",
            "\n",
            "Epoch 117/1000\n",
            "40/40 [==============================]\n",
            " - 0s 742us/step - loss: 0.0375 - val_loss: 0.5768\n",
            "\n",
            "Epoch 118/1000\n",
            "40/40 [==============================]\n",
            " - 0s 923us/step - loss: 0.0415 - val_loss: 0.5768\n",
            "\n",
            "Epoch 119/1000\n",
            "40/40 [==============================]\n",
            " - 0s 940us/step - loss: 0.0434 - val_loss: 0.5772\n",
            "\n",
            "Epoch 120/1000\n",
            "40/40 [==============================]\n",
            " - 0s 828us/step - loss: 0.0463 - val_loss: 0.5772\n",
            "\n",
            "Epoch 121/1000\n",
            "40/40 [==============================]\n",
            " - 0s 712us/step - loss: 0.0400 - val_loss: 0.5771\n",
            "\n",
            "Epoch 122/1000\n",
            "40/40 [==============================]\n",
            " - 0s 927us/step - loss: 0.0422 - val_loss: 0.5764\n",
            "\n",
            "Epoch 123/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0432 - val_loss: 0.5745\n",
            "\n",
            "Epoch 124/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0430 - val_loss: 0.5722\n",
            "\n",
            "Epoch 125/1000\n",
            "40/40 [==============================]\n",
            " - 0s 865us/step - loss: 0.0391 - val_loss: 0.5705\n",
            "\n",
            "Epoch 126/1000\n",
            "40/40 [==============================]\n",
            " - 0s 893us/step - loss: 0.0412 - val_loss: 0.5696\n",
            "\n",
            "Epoch 127/1000\n",
            "40/40 [==============================]\n",
            " - 0s 946us/step - loss: 0.0455 - val_loss: 0.5692\n",
            "\n",
            "Epoch 128/1000\n",
            "40/40 [==============================]\n",
            " - 0s 916us/step - loss: 0.0412 - val_loss: 0.5691\n",
            "\n",
            "Epoch 129/1000\n",
            "40/40 [==============================]\n",
            " - 0s 779us/step - loss: 0.0417 - val_loss: 0.5699\n",
            "\n",
            "Epoch 130/1000\n",
            "40/40 [==============================]\n",
            " - 0s 875us/step - loss: 0.0364 - val_loss: 0.5710\n",
            "\n",
            "Epoch 131/1000\n",
            "40/40 [==============================]\n",
            " - 0s 782us/step - loss: 0.0444 - val_loss: 0.5732\n",
            "\n",
            "Epoch 132/1000\n",
            "40/40 [==============================]\n",
            " - 0s 991us/step - loss: 0.0429 - val_loss: 0.5751\n",
            "\n",
            "Epoch 133/1000\n",
            "40/40 [==============================]\n",
            " - 0s 783us/step - loss: 0.0366 - val_loss: 0.5763\n",
            "\n",
            "Epoch 134/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0432 - val_loss: 0.5773\n",
            "\n",
            "Epoch 135/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0425 - val_loss: 0.5777\n",
            "\n",
            "Epoch 136/1000\n",
            "40/40 [==============================]\n",
            " - 0s 832us/step - loss: 0.0397 - val_loss: 0.5793\n",
            "\n",
            "Epoch 137/1000\n",
            "40/40 [==============================]\n",
            " - 0s 822us/step - loss: 0.0371 - val_loss: 0.5816\n",
            "\n",
            "Epoch 138/1000\n",
            "40/40 [==============================]\n",
            " - 0s 914us/step - loss: 0.0462 - val_loss: 0.5841\n",
            "\n",
            "Epoch 139/1000\n",
            "40/40 [==============================]\n",
            " - 0s 985us/step - loss: 0.0411 - val_loss: 0.5875\n",
            "\n",
            "Epoch 140/1000\n",
            "40/40 [==============================]\n",
            " - 0s 901us/step - loss: 0.0358 - val_loss: 0.5895\n",
            "\n",
            "Epoch 141/1000\n",
            "40/40 [==============================]\n",
            " - 0s 876us/step - loss: 0.0416 - val_loss: 0.5919\n",
            "\n",
            "Epoch 142/1000\n",
            "40/40 [==============================]\n",
            " - 0s 732us/step - loss: 0.0355 - val_loss: 0.5931\n",
            "\n",
            "Epoch 143/1000\n",
            "40/40 [==============================]\n",
            " - 0s 976us/step - loss: 0.0375 - val_loss: 0.5937\n",
            "\n",
            "Epoch 144/1000\n",
            "40/40 [==============================]\n",
            " - 0s 800us/step - loss: 0.0409 - val_loss: 0.5935\n",
            "\n",
            "Epoch 145/1000\n",
            "40/40 [==============================]\n",
            " - 0s 978us/step - loss: 0.0408 - val_loss: 0.5942\n",
            "\n",
            "Epoch 146/1000\n",
            "40/40 [==============================]\n",
            " - 0s 948us/step - loss: 0.0438 - val_loss: 0.5956\n",
            "\n",
            "Epoch 147/1000\n",
            "40/40 [==============================]\n",
            " - 0s 927us/step - loss: 0.0382 - val_loss: 0.5958\n",
            "\n",
            "Epoch 148/1000\n",
            "40/40 [==============================]\n",
            " - 0s 972us/step - loss: 0.0365 - val_loss: 0.5954\n",
            "\n",
            "Epoch 149/1000\n",
            "40/40 [==============================]\n",
            " - 0s 796us/step - loss: 0.0355 - val_loss: 0.5951\n",
            "\n",
            "Epoch 150/1000\n",
            "40/40 [==============================]\n",
            " - 0s 844us/step - loss: 0.0398 - val_loss: 0.5946\n",
            "\n",
            "Epoch 151/1000\n",
            "40/40 [==============================]\n",
            " - 0s 976us/step - loss: 0.0440 - val_loss: 0.5943\n",
            "\n",
            "Epoch 152/1000\n",
            "40/40 [==============================]\n",
            " - 0s 739us/step - loss: 0.0461 - val_loss: 0.5953\n",
            "\n",
            "Epoch 153/1000\n",
            "40/40 [==============================]\n",
            " - 0s 728us/step - loss: 0.0393 - val_loss: 0.5968\n",
            "\n",
            "Epoch 154/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0401 - val_loss: 0.5983\n",
            "\n",
            "Epoch 155/1000\n",
            "40/40 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0370 - val_loss: 0.5994\n",
            "\n",
            "Epoch 156/1000\n",
            "40/40 [==============================]\n",
            " - 0s 759us/step - loss: 0.0426 - val_loss: 0.5999\n",
            "\n",
            "Epoch 157/1000\n",
            "40/40 [==============================]\n",
            " - 0s 849us/step - loss: 0.0349 - val_loss: 0.6008\n",
            "\n",
            "Epoch 158/1000\n",
            "40/40 [==============================]\n",
            " - 0s 873us/step - loss: 0.0394 - val_loss: 0.6016\n",
            "\n",
            "Epoch 159/1000\n",
            "40/40 [==============================]\n",
            " - 0s 610us/step - loss: 0.0356 - val_loss: 0.6021\n",
            "\n",
            "Epoch 160/1000\n",
            "40/40 [==============================]\n",
            " - 0s 903us/step - loss: 0.0369 - val_loss: 0.6017\n",
            "\n",
            "Epoch 161/1000\n",
            "40/40 [==============================]\n",
            " - 0s 793us/step - loss: 0.0391 - val_loss: 0.6009\n",
            "\n",
            "Epoch 162/1000\n",
            "40/40 [==============================]\n",
            " - 0s 913us/step - loss: 0.0333 - val_loss: 0.5996\n",
            "\n",
            "Epoch 163/1000\n",
            "40/40 [==============================]\n",
            " - 0s 982us/step - loss: 0.0425 - val_loss: 0.5990\n",
            "\n",
            "51/51 [==============================]\n",
            " - 0s 84us/step\n",
            "\n",
            "Window size 1 score = 0.1557055115699768\n",
            "Window size is 7\n",
            "  6%|▌         | 6/100 [02:52<46:48, 29.88s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1991-12-03</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>1.698857</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>1.701144</td>\n",
              "      <td>1.711261</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1991-07-23</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.347058</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.359611</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1991-09-17</td>\n",
              "      <td>0.813020</td>\n",
              "      <td>0.877991</td>\n",
              "      <td>0.851469</td>\n",
              "      <td>0.847470</td>\n",
              "      <td>0.813020</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1991-07-09</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.295861</td>\n",
              "      <td>0.282524</td>\n",
              "      <td>0.289122</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1991-08-20</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.621362</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.631832</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1991-09-03</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.805993</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.815060</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1991-03-12</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.232593</td>\n",
              "      <td>0.220689</td>\n",
              "      <td>0.202589</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1991-03-05</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.261587</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.251792</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1991-10-08</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.865488</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.874104</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "48 1991-12-03  1.711261  1.698857  1.711261  1.701144   1.711261     0.0\n",
              "29 1991-07-23  0.354998  0.347058  0.354998  0.359611   0.354998     0.0\n",
              "37 1991-09-17  0.813020  0.877991  0.851469  0.847470   0.813020     0.0\n",
              "27 1991-07-09  0.266723  0.295861  0.282524  0.289122   0.266723     0.0\n",
              "33 1991-08-20  0.670915  0.621362  0.670915  0.631832   0.670915     0.0\n",
              "35 1991-09-03  0.857685  0.805993  0.857685  0.815060   0.857685     0.0\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "10 1991-03-12  0.177921  0.232593  0.220689  0.202589   0.177921     0.0\n",
              "9  1991-03-05  0.226377  0.261587  0.249026  0.251792   0.226377     0.0\n",
              "40 1991-10-08  0.914885  0.865488  0.914885  0.874104   0.914885     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36 samples, validate on 9 samples\n",
            "Epoch 1/1000\n",
            "36/36 [==============================]\n",
            " - 9s 257ms/step - loss: 0.3180 - val_loss: 1.0853\n",
            "\n",
            "Epoch 2/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3164 - val_loss: 1.0824\n",
            "\n",
            "Epoch 3/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3150 - val_loss: 1.0784\n",
            "\n",
            "Epoch 4/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3129 - val_loss: 1.0725\n",
            "\n",
            "Epoch 5/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3102 - val_loss: 1.0633\n",
            "\n",
            "Epoch 6/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3080 - val_loss: 1.0495\n",
            "\n",
            "Epoch 7/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3038 - val_loss: 1.0290\n",
            "\n",
            "Epoch 8/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2979 - val_loss: 0.9994\n",
            "\n",
            "Epoch 9/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2914 - val_loss: 0.9575\n",
            "\n",
            "Epoch 10/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2825 - val_loss: 0.8996\n",
            "\n",
            "Epoch 11/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2703 - val_loss: 0.8228\n",
            "\n",
            "Epoch 12/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2548 - val_loss: 0.7247\n",
            "\n",
            "Epoch 13/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2320 - val_loss: 0.6360\n",
            "\n",
            "Epoch 14/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2056 - val_loss: 0.5978\n",
            "\n",
            "Epoch 15/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1835 - val_loss: 0.5978\n",
            "\n",
            "Epoch 16/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1764 - val_loss: 0.5978\n",
            "\n",
            "Epoch 17/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1539 - val_loss: 0.5978\n",
            "\n",
            "Epoch 18/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1557 - val_loss: 0.5978\n",
            "\n",
            "Epoch 19/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1485 - val_loss: 0.5978\n",
            "\n",
            "Epoch 20/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1495 - val_loss: 0.5978\n",
            "\n",
            "Epoch 21/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1584 - val_loss: 0.5978\n",
            "\n",
            "Epoch 22/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1610 - val_loss: 0.5978\n",
            "\n",
            "Epoch 23/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1553 - val_loss: 0.5978\n",
            "\n",
            "Epoch 24/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1529 - val_loss: 0.5978\n",
            "\n",
            "Epoch 25/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1532 - val_loss: 0.5978\n",
            "\n",
            "Epoch 26/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1520 - val_loss: 0.5978\n",
            "\n",
            "Epoch 27/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1473 - val_loss: 0.5978\n",
            "\n",
            "Epoch 28/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1427 - val_loss: 0.5978\n",
            "\n",
            "Epoch 29/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1512 - val_loss: 0.5978\n",
            "\n",
            "Epoch 30/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1507 - val_loss: 0.5978\n",
            "\n",
            "Epoch 31/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1484 - val_loss: 0.5978\n",
            "\n",
            "Epoch 32/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1498 - val_loss: 0.5978\n",
            "\n",
            "Epoch 33/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1477 - val_loss: 0.5978\n",
            "\n",
            "Epoch 34/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1467 - val_loss: 0.5978\n",
            "\n",
            "Epoch 35/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1462 - val_loss: 0.5978\n",
            "\n",
            "Epoch 36/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1461 - val_loss: 0.5978\n",
            "\n",
            "Epoch 37/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1466 - val_loss: 0.5978\n",
            "\n",
            "Epoch 38/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1469 - val_loss: 0.5978\n",
            "\n",
            "Epoch 39/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1385 - val_loss: 0.5978\n",
            "\n",
            "Epoch 40/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1386 - val_loss: 0.5978\n",
            "\n",
            "Epoch 41/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1344 - val_loss: 0.5978\n",
            "\n",
            "Epoch 42/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1396 - val_loss: 0.5978\n",
            "\n",
            "Epoch 43/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1384 - val_loss: 0.5978\n",
            "\n",
            "Epoch 44/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1408 - val_loss: 0.5978\n",
            "\n",
            "Epoch 45/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1332 - val_loss: 0.5978\n",
            "\n",
            "Epoch 46/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1249 - val_loss: 0.5978\n",
            "\n",
            "Epoch 47/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1266 - val_loss: 0.5978\n",
            "\n",
            "Epoch 48/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1272 - val_loss: 0.5978\n",
            "\n",
            "Epoch 49/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1259 - val_loss: 0.5978\n",
            "\n",
            "Epoch 50/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5978\n",
            "\n",
            "Epoch 51/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1244 - val_loss: 0.5978\n",
            "\n",
            "Epoch 52/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1241 - val_loss: 0.5978\n",
            "\n",
            "Epoch 53/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1225 - val_loss: 0.5978\n",
            "\n",
            "Epoch 54/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1286 - val_loss: 0.5978\n",
            "\n",
            "Epoch 55/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1219 - val_loss: 0.5978\n",
            "\n",
            "Epoch 56/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1350 - val_loss: 0.5978\n",
            "\n",
            "Epoch 57/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1335 - val_loss: 0.5978\n",
            "\n",
            "Epoch 58/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1219 - val_loss: 0.5978\n",
            "\n",
            "Epoch 59/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1389 - val_loss: 0.5978\n",
            "\n",
            "Epoch 60/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1283 - val_loss: 0.5978\n",
            "\n",
            "Epoch 61/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5978\n",
            "\n",
            "Epoch 62/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1346 - val_loss: 0.5978\n",
            "\n",
            "Epoch 63/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1254 - val_loss: 0.5978\n",
            "\n",
            "Epoch 64/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1286 - val_loss: 0.5978\n",
            "\n",
            "Epoch 65/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1252 - val_loss: 0.5978\n",
            "\n",
            "Epoch 66/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1293 - val_loss: 0.5978\n",
            "\n",
            "Epoch 67/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1242 - val_loss: 0.5978\n",
            "\n",
            "Epoch 68/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1231 - val_loss: 0.5978\n",
            "\n",
            "Epoch 69/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1193 - val_loss: 0.5978\n",
            "\n",
            "Epoch 70/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1303 - val_loss: 0.6124\n",
            "\n",
            "Epoch 71/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1292 - val_loss: 0.6395\n",
            "\n",
            "Epoch 72/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1253 - val_loss: 0.6546\n",
            "\n",
            "Epoch 73/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1221 - val_loss: 0.6754\n",
            "\n",
            "Epoch 74/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1220 - val_loss: 0.6771\n",
            "\n",
            "Epoch 75/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1213 - val_loss: 0.6899\n",
            "\n",
            "Epoch 76/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1270 - val_loss: 0.7108\n",
            "\n",
            "Epoch 77/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1202 - val_loss: 0.7248\n",
            "\n",
            "Epoch 78/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1133 - val_loss: 0.7349\n",
            "\n",
            "Epoch 79/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1188 - val_loss: 0.7419\n",
            "\n",
            "Epoch 80/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1137 - val_loss: 0.7472\n",
            "\n",
            "Epoch 81/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1152 - val_loss: 0.7499\n",
            "\n",
            "Epoch 82/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1230 - val_loss: 0.7502\n",
            "\n",
            "Epoch 83/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1161 - val_loss: 0.7462\n",
            "\n",
            "Epoch 84/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1113 - val_loss: 0.7353\n",
            "\n",
            "Epoch 85/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1128 - val_loss: 0.7218\n",
            "\n",
            "Epoch 86/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1142 - val_loss: 0.7106\n",
            "\n",
            "Epoch 87/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1058 - val_loss: 0.6997\n",
            "\n",
            "Epoch 88/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1125 - val_loss: 0.6874\n",
            "\n",
            "Epoch 89/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1099 - val_loss: 0.6706\n",
            "\n",
            "Epoch 90/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1065 - val_loss: 0.6562\n",
            "\n",
            "Epoch 91/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1040 - val_loss: 0.6515\n",
            "\n",
            "Epoch 92/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1061 - val_loss: 0.6414\n",
            "\n",
            "Epoch 93/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1055 - val_loss: 0.6446\n",
            "\n",
            "Epoch 94/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1077 - val_loss: 0.6527\n",
            "\n",
            "Epoch 95/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1123 - val_loss: 0.6682\n",
            "\n",
            "Epoch 96/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1051 - val_loss: 0.6885\n",
            "\n",
            "Epoch 97/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1071 - val_loss: 0.7065\n",
            "\n",
            "Epoch 98/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1116 - val_loss: 0.7220\n",
            "\n",
            "Epoch 99/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1080 - val_loss: 0.7251\n",
            "\n",
            "Epoch 100/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1063 - val_loss: 0.7164\n",
            "\n",
            "Epoch 101/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1068 - val_loss: 0.6978\n",
            "\n",
            "Epoch 102/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1041 - val_loss: 0.6819\n",
            "\n",
            "Epoch 103/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0982 - val_loss: 0.6616\n",
            "\n",
            "Epoch 104/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1072 - val_loss: 0.6391\n",
            "\n",
            "Epoch 105/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1053 - val_loss: 0.6126\n",
            "\n",
            "Epoch 106/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1020 - val_loss: 0.6016\n",
            "\n",
            "Epoch 107/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0968 - val_loss: 0.5991\n",
            "\n",
            "Epoch 108/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1082 - val_loss: 0.6029\n",
            "\n",
            "Epoch 109/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1106 - val_loss: 0.6095\n",
            "\n",
            "Epoch 110/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1026 - val_loss: 0.6244\n",
            "\n",
            "Epoch 111/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1060 - val_loss: 0.6325\n",
            "\n",
            "Epoch 112/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1147 - val_loss: 0.6525\n",
            "\n",
            "Epoch 113/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1000 - val_loss: 0.6736\n",
            "\n",
            "Epoch 114/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1069 - val_loss: 0.6869\n",
            "\n",
            "45/45 [==============================]\n",
            " - 0s 273us/step\n",
            "\n",
            "Window size 7 score = 0.22676366567611694\n",
            "Window size is 5\n",
            "  7%|▋         | 7/100 [03:30<50:01, 32.27s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1991-08-20</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.621362</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.631832</td>\n",
              "      <td>0.670915</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1991-12-10</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.786161</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>1.787785</td>\n",
              "      <td>1.791109</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1991-11-26</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.571244</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.574500</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1991-05-28</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.048394</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.063215</td>\n",
              "      <td>0.085853</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1991-08-27</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.711360</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.721147</td>\n",
              "      <td>0.762667</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1991-06-25</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.283466</td>\n",
              "      <td>0.313283</td>\n",
              "      <td>0.296502</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1991-05-21</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1991-06-04</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.107027</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.121403</td>\n",
              "      <td>0.145897</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1991-09-24</td>\n",
              "      <td>0.770041</td>\n",
              "      <td>0.828735</td>\n",
              "      <td>0.803329</td>\n",
              "      <td>0.803829</td>\n",
              "      <td>0.770041</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1991-07-09</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.295861</td>\n",
              "      <td>0.282524</td>\n",
              "      <td>0.289122</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "33 1991-08-20  0.670915  0.621362  0.670915  0.631832   0.670915     0.0\n",
              "49 1991-12-10  1.791109  1.786161  1.791109  1.787785   1.791109     0.0\n",
              "47 1991-11-26  1.626146  1.571244  1.626146  1.574500   1.626146     0.0\n",
              "21 1991-05-28  0.085853  0.048394  0.085853  0.063215   0.085853     0.0\n",
              "34 1991-08-27  0.762667  0.711360  0.762667  0.721147   0.762667     0.0\n",
              "25 1991-06-25  0.305804  0.283466  0.313283  0.296502   0.305804     0.0\n",
              "20 1991-05-21  0.029917  0.017461  0.029917  0.026527   0.029917     0.0\n",
              "22 1991-06-04  0.145897  0.107027  0.145897  0.121403   0.145897     0.0\n",
              "38 1991-09-24  0.770041  0.828735  0.803329  0.803829   0.770041     0.0\n",
              "27 1991-07-09  0.266723  0.295861  0.282524  0.289122   0.266723     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 37 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "37/37 [==============================]\n",
            " - 10s 275ms/step - loss: 0.3175 - val_loss: 1.0202\n",
            "\n",
            "Epoch 2/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.3160 - val_loss: 1.0214\n",
            "\n",
            "Epoch 3/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3146 - val_loss: 1.0228\n",
            "\n",
            "Epoch 4/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3130 - val_loss: 1.0240\n",
            "\n",
            "Epoch 5/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3109 - val_loss: 1.0249\n",
            "\n",
            "Epoch 6/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.3082 - val_loss: 1.0255\n",
            "\n",
            "Epoch 7/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3054 - val_loss: 1.0253\n",
            "\n",
            "Epoch 8/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3020 - val_loss: 1.0241\n",
            "\n",
            "Epoch 9/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2975 - val_loss: 1.0208\n",
            "\n",
            "Epoch 10/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2930 - val_loss: 1.0147\n",
            "\n",
            "Epoch 11/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2863 - val_loss: 1.0047\n",
            "\n",
            "Epoch 12/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2794 - val_loss: 0.9894\n",
            "\n",
            "Epoch 13/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2706 - val_loss: 0.9675\n",
            "\n",
            "Epoch 14/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2581 - val_loss: 0.9370\n",
            "\n",
            "Epoch 15/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2438 - val_loss: 0.8964\n",
            "\n",
            "Epoch 16/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2264 - val_loss: 0.8451\n",
            "\n",
            "Epoch 17/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2019 - val_loss: 0.7830\n",
            "\n",
            "Epoch 18/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1765 - val_loss: 0.7121\n",
            "\n",
            "Epoch 19/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1517 - val_loss: 0.6428\n",
            "\n",
            "Epoch 20/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1331 - val_loss: 0.5923\n",
            "\n",
            "Epoch 21/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1164 - val_loss: 0.5613\n",
            "\n",
            "Epoch 22/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1140 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1136 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1136 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1086 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1248 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1302 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1181 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1068 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1150 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1218 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1120 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1056 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1049 - val_loss: 0.5498\n",
            "\n",
            "Epoch 35/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1174 - val_loss: 0.5498\n",
            "\n",
            "Epoch 36/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1073 - val_loss: 0.5498\n",
            "\n",
            "Epoch 37/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1130 - val_loss: 0.5498\n",
            "\n",
            "Epoch 38/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1067 - val_loss: 0.5498\n",
            "\n",
            "Epoch 39/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1088 - val_loss: 0.5498\n",
            "\n",
            "Epoch 40/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1035 - val_loss: 0.5524\n",
            "\n",
            "Epoch 41/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.5663\n",
            "\n",
            "Epoch 42/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0992 - val_loss: 0.5892\n",
            "\n",
            "Epoch 43/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1024 - val_loss: 0.6142\n",
            "\n",
            "Epoch 44/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0978 - val_loss: 0.6359\n",
            "\n",
            "Epoch 45/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1012 - val_loss: 0.6547\n",
            "\n",
            "Epoch 46/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1018 - val_loss: 0.6711\n",
            "\n",
            "Epoch 47/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0987 - val_loss: 0.6841\n",
            "\n",
            "Epoch 48/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1016 - val_loss: 0.6944\n",
            "\n",
            "Epoch 49/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7034\n",
            "\n",
            "Epoch 50/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0989 - val_loss: 0.7066\n",
            "\n",
            "Epoch 51/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0989 - val_loss: 0.7093\n",
            "\n",
            "Epoch 52/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0958 - val_loss: 0.7121\n",
            "\n",
            "Epoch 53/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0980 - val_loss: 0.7148\n",
            "\n",
            "Epoch 54/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0981 - val_loss: 0.7173\n",
            "\n",
            "Epoch 55/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0941 - val_loss: 0.7207\n",
            "\n",
            "Epoch 56/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7259\n",
            "\n",
            "Epoch 57/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0958 - val_loss: 0.7304\n",
            "\n",
            "Epoch 58/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1005 - val_loss: 0.7355\n",
            "\n",
            "Epoch 59/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0960 - val_loss: 0.7378\n",
            "\n",
            "Epoch 60/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0944 - val_loss: 0.7418\n",
            "\n",
            "Epoch 61/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0921 - val_loss: 0.7441\n",
            "\n",
            "Epoch 62/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7444\n",
            "\n",
            "Epoch 63/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.7482\n",
            "\n",
            "Epoch 64/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0945 - val_loss: 0.7517\n",
            "\n",
            "Epoch 65/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0963 - val_loss: 0.7469\n",
            "\n",
            "Epoch 66/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0924 - val_loss: 0.7440\n",
            "\n",
            "Epoch 67/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0907 - val_loss: 0.7449\n",
            "\n",
            "Epoch 68/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0971 - val_loss: 0.7486\n",
            "\n",
            "Epoch 69/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0931 - val_loss: 0.7557\n",
            "\n",
            "Epoch 70/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0895 - val_loss: 0.7652\n",
            "\n",
            "Epoch 71/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0986 - val_loss: 0.7749\n",
            "\n",
            "Epoch 72/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0998 - val_loss: 0.7840\n",
            "\n",
            "Epoch 73/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0981 - val_loss: 0.7947\n",
            "\n",
            "Epoch 74/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0945 - val_loss: 0.7981\n",
            "\n",
            "Epoch 75/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1021 - val_loss: 0.7955\n",
            "\n",
            "Epoch 76/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0992 - val_loss: 0.7875\n",
            "\n",
            "Epoch 77/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0974 - val_loss: 0.7689\n",
            "\n",
            "Epoch 78/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0913 - val_loss: 0.7522\n",
            "\n",
            "Epoch 79/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0973 - val_loss: 0.7415\n",
            "\n",
            "Epoch 80/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0904 - val_loss: 0.7326\n",
            "\n",
            "Epoch 81/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0949 - val_loss: 0.7263\n",
            "\n",
            "Epoch 82/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0896 - val_loss: 0.7211\n",
            "\n",
            "Epoch 83/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0862 - val_loss: 0.7189\n",
            "\n",
            "Epoch 84/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0922 - val_loss: 0.7188\n",
            "\n",
            "Epoch 85/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0957 - val_loss: 0.7192\n",
            "\n",
            "Epoch 86/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.7204\n",
            "\n",
            "Epoch 87/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0879 - val_loss: 0.7223\n",
            "\n",
            "Epoch 88/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0900 - val_loss: 0.7265\n",
            "\n",
            "Epoch 89/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0903 - val_loss: 0.7315\n",
            "\n",
            "Epoch 90/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0912 - val_loss: 0.7387\n",
            "\n",
            "Epoch 91/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0846 - val_loss: 0.7382\n",
            "\n",
            "Epoch 92/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0930 - val_loss: 0.7400\n",
            "\n",
            "Epoch 93/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0932 - val_loss: 0.7421\n",
            "\n",
            "Epoch 94/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0885 - val_loss: 0.7396\n",
            "\n",
            "Epoch 95/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0934 - val_loss: 0.7300\n",
            "\n",
            "Epoch 96/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0898 - val_loss: 0.7253\n",
            "\n",
            "Epoch 97/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0872 - val_loss: 0.7231\n",
            "\n",
            "Epoch 98/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0873 - val_loss: 0.7138\n",
            "\n",
            "Epoch 99/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0941 - val_loss: 0.7074\n",
            "\n",
            "Epoch 100/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0815 - val_loss: 0.7029\n",
            "\n",
            "Epoch 101/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0891 - val_loss: 0.7015\n",
            "\n",
            "Epoch 102/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0896 - val_loss: 0.7088\n",
            "\n",
            "Epoch 103/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0851 - val_loss: 0.7172\n",
            "\n",
            "Epoch 104/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0889 - val_loss: 0.7245\n",
            "\n",
            "Epoch 105/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0847 - val_loss: 0.7204\n",
            "\n",
            "Epoch 106/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0920 - val_loss: 0.7161\n",
            "\n",
            "Epoch 107/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0865 - val_loss: 0.7162\n",
            "\n",
            "Epoch 108/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0907 - val_loss: 0.7156\n",
            "\n",
            "Epoch 109/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0809 - val_loss: 0.7091\n",
            "\n",
            "Epoch 110/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0864 - val_loss: 0.7052\n",
            "\n",
            "Epoch 111/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0901 - val_loss: 0.7014\n",
            "\n",
            "Epoch 112/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0803 - val_loss: 0.6971\n",
            "\n",
            "Epoch 113/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0886 - val_loss: 0.6931\n",
            "\n",
            "Epoch 114/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0850 - val_loss: 0.6944\n",
            "\n",
            "Epoch 115/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0877 - val_loss: 0.6933\n",
            "\n",
            "Epoch 116/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0866 - val_loss: 0.6911\n",
            "\n",
            "Epoch 117/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0848 - val_loss: 0.6912\n",
            "\n",
            "Epoch 118/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0871 - val_loss: 0.6936\n",
            "\n",
            "Epoch 119/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0867 - val_loss: 0.6988\n",
            "\n",
            "Epoch 120/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0835 - val_loss: 0.6963\n",
            "\n",
            "Epoch 121/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0775 - val_loss: 0.6956\n",
            "\n",
            "Epoch 122/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0840 - val_loss: 0.7093\n",
            "\n",
            "47/47 [==============================]\n",
            " - 0s 188us/step\n",
            "\n",
            "Window size 5 score = 0.21506617963314056\n",
            "Window size is 7\n",
            "  8%|▊         | 8/100 [04:09<52:38, 34.33s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1991-03-19</td>\n",
              "      <td>0.150637</td>\n",
              "      <td>0.183552</td>\n",
              "      <td>0.172759</td>\n",
              "      <td>0.174885</td>\n",
              "      <td>0.150637</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1991-11-26</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.571244</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>1.574500</td>\n",
              "      <td>1.626146</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1991-06-11</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.169217</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.183121</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1991-02-26</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.288209</td>\n",
              "      <td>0.275045</td>\n",
              "      <td>0.280458</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1991-08-13</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.537616</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.548722</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1991-01-01</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1991-10-22</td>\n",
              "      <td>1.118298</td>\n",
              "      <td>1.070705</td>\n",
              "      <td>1.118298</td>\n",
              "      <td>1.077762</td>\n",
              "      <td>1.118298</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1991-02-12</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.273612</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991-01-08</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.274736</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.287838</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1991-07-30</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.379392</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.391700</td>\n",
              "      <td>0.424839</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "11 1991-03-19  0.150637  0.183552  0.172759  0.174885   0.150637     0.0\n",
              "47 1991-11-26  1.626146  1.571244  1.626146  1.574500   1.626146     0.0\n",
              "23 1991-06-11  0.189087  0.169217  0.189087  0.183121   0.189087     0.0\n",
              "8  1991-02-26  0.254609  0.288209  0.275045  0.280458   0.254609     0.0\n",
              "32 1991-08-13  0.583483  0.537616  0.583483  0.548722   0.583483     0.0\n",
              "0  1991-01-01  0.255346  0.233348  0.255346  0.246764   0.255346     0.0\n",
              "42 1991-10-22  1.118298  1.070705  1.118298  1.077762   1.118298     0.0\n",
              "6  1991-02-12  0.260297  0.260401  0.260297  0.273612   0.260297     0.0\n",
              "1  1991-01-08  0.282840  0.274736  0.282840  0.287838   0.282840     0.0\n",
              "30 1991-07-30  0.424839  0.379392  0.424839  0.391700   0.424839     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36 samples, validate on 9 samples\n",
            "Epoch 1/1000\n",
            "36/36 [==============================]\n",
            " - 11s 316ms/step - loss: 0.2969 - val_loss: 1.1530\n",
            "\n",
            "Epoch 2/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2959 - val_loss: 1.1505\n",
            "\n",
            "Epoch 3/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2946 - val_loss: 1.1474\n",
            "\n",
            "Epoch 4/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2933 - val_loss: 1.1433\n",
            "\n",
            "Epoch 5/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2916 - val_loss: 1.1373\n",
            "\n",
            "Epoch 6/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2894 - val_loss: 1.1285\n",
            "\n",
            "Epoch 7/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2866 - val_loss: 1.1155\n",
            "\n",
            "Epoch 8/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2829 - val_loss: 1.0964\n",
            "\n",
            "Epoch 9/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2784 - val_loss: 1.0690\n",
            "\n",
            "Epoch 10/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2722 - val_loss: 1.0305\n",
            "\n",
            "Epoch 11/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2619 - val_loss: 0.9785\n",
            "\n",
            "Epoch 12/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2506 - val_loss: 0.9096\n",
            "\n",
            "Epoch 13/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2357 - val_loss: 0.8219\n",
            "\n",
            "Epoch 14/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2192 - val_loss: 0.7167\n",
            "\n",
            "Epoch 15/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1901 - val_loss: 0.6273\n",
            "\n",
            "Epoch 16/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1624 - val_loss: 0.5978\n",
            "\n",
            "Epoch 17/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1562 - val_loss: 0.5978\n",
            "\n",
            "Epoch 18/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1551 - val_loss: 0.5978\n",
            "\n",
            "Epoch 19/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1597 - val_loss: 0.5978\n",
            "\n",
            "Epoch 20/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1560 - val_loss: 0.5978\n",
            "\n",
            "Epoch 21/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1510 - val_loss: 0.5978\n",
            "\n",
            "Epoch 22/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1359 - val_loss: 0.5978\n",
            "\n",
            "Epoch 23/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1401 - val_loss: 0.5978\n",
            "\n",
            "Epoch 24/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1407 - val_loss: 0.5978\n",
            "\n",
            "Epoch 25/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1298 - val_loss: 0.5978\n",
            "\n",
            "Epoch 26/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1472 - val_loss: 0.5978\n",
            "\n",
            "Epoch 27/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1465 - val_loss: 0.5978\n",
            "\n",
            "Epoch 28/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1312 - val_loss: 0.5978\n",
            "\n",
            "Epoch 29/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1239 - val_loss: 0.5978\n",
            "\n",
            "Epoch 30/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1362 - val_loss: 0.5978\n",
            "\n",
            "Epoch 31/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1449 - val_loss: 0.5978\n",
            "\n",
            "Epoch 32/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1196 - val_loss: 0.5978\n",
            "\n",
            "Epoch 33/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1307 - val_loss: 0.5978\n",
            "\n",
            "Epoch 34/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1311 - val_loss: 0.5978\n",
            "\n",
            "Epoch 35/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1211 - val_loss: 0.5978\n",
            "\n",
            "Epoch 36/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1231 - val_loss: 0.5978\n",
            "\n",
            "Epoch 37/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1279 - val_loss: 0.5978\n",
            "\n",
            "Epoch 38/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1244 - val_loss: 0.5978\n",
            "\n",
            "Epoch 39/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1250 - val_loss: 0.5978\n",
            "\n",
            "Epoch 40/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1245 - val_loss: 0.5978\n",
            "\n",
            "Epoch 41/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1230 - val_loss: 0.5978\n",
            "\n",
            "Epoch 42/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1189 - val_loss: 0.5978\n",
            "\n",
            "Epoch 43/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1235 - val_loss: 0.5978\n",
            "\n",
            "Epoch 44/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1275 - val_loss: 0.5978\n",
            "\n",
            "Epoch 45/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1289 - val_loss: 0.5978\n",
            "\n",
            "Epoch 46/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1231 - val_loss: 0.5978\n",
            "\n",
            "Epoch 47/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1281 - val_loss: 0.5978\n",
            "\n",
            "Epoch 48/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1288 - val_loss: 0.5978\n",
            "\n",
            "Epoch 49/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1226 - val_loss: 0.5978\n",
            "\n",
            "Epoch 50/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1199 - val_loss: 0.5978\n",
            "\n",
            "Epoch 51/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1326 - val_loss: 0.5978\n",
            "\n",
            "Epoch 52/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1298 - val_loss: 0.5978\n",
            "\n",
            "Epoch 53/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1275 - val_loss: 0.5978\n",
            "\n",
            "Epoch 54/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1258 - val_loss: 0.5978\n",
            "\n",
            "Epoch 55/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1247 - val_loss: 0.5978\n",
            "\n",
            "Epoch 56/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1263 - val_loss: 0.5978\n",
            "\n",
            "Epoch 57/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1311 - val_loss: 0.5978\n",
            "\n",
            "Epoch 58/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1238 - val_loss: 0.5978\n",
            "\n",
            "Epoch 59/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1291 - val_loss: 0.5978\n",
            "\n",
            "Epoch 60/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1171 - val_loss: 0.5978\n",
            "\n",
            "Epoch 61/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1164 - val_loss: 0.5978\n",
            "\n",
            "Epoch 62/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1219 - val_loss: 0.5978\n",
            "\n",
            "Epoch 63/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1289 - val_loss: 0.5978\n",
            "\n",
            "Epoch 64/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1216 - val_loss: 0.5978\n",
            "\n",
            "Epoch 65/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1150 - val_loss: 0.5978\n",
            "\n",
            "Epoch 66/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1227 - val_loss: 0.5978\n",
            "\n",
            "Epoch 67/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1330 - val_loss: 0.5978\n",
            "\n",
            "Epoch 68/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1256 - val_loss: 0.5978\n",
            "\n",
            "Epoch 69/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1238 - val_loss: 0.5978\n",
            "\n",
            "Epoch 70/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1247 - val_loss: 0.5978\n",
            "\n",
            "Epoch 71/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1184 - val_loss: 0.5978\n",
            "\n",
            "Epoch 72/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1248 - val_loss: 0.5978\n",
            "\n",
            "Epoch 73/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1244 - val_loss: 0.5978\n",
            "\n",
            "Epoch 74/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1180 - val_loss: 0.5978\n",
            "\n",
            "Epoch 75/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1232 - val_loss: 0.5978\n",
            "\n",
            "Epoch 76/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1195 - val_loss: 0.5978\n",
            "\n",
            "Epoch 77/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1237 - val_loss: 0.5978\n",
            "\n",
            "Epoch 78/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1239 - val_loss: 0.5978\n",
            "\n",
            "Epoch 79/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1186 - val_loss: 0.5978\n",
            "\n",
            "Epoch 80/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1240 - val_loss: 0.5978\n",
            "\n",
            "Epoch 81/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1129 - val_loss: 0.5978\n",
            "\n",
            "Epoch 82/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1183 - val_loss: 0.5978\n",
            "\n",
            "Epoch 83/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1216 - val_loss: 0.5978\n",
            "\n",
            "Epoch 84/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1145 - val_loss: 0.5978\n",
            "\n",
            "Epoch 85/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1215 - val_loss: 0.5978\n",
            "\n",
            "Epoch 86/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1194 - val_loss: 0.5978\n",
            "\n",
            "Epoch 87/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1220 - val_loss: 0.5978\n",
            "\n",
            "Epoch 88/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1221 - val_loss: 0.5978\n",
            "\n",
            "Epoch 89/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1159 - val_loss: 0.5978\n",
            "\n",
            "Epoch 90/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1146 - val_loss: 0.5978\n",
            "\n",
            "Epoch 91/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1234 - val_loss: 0.5978\n",
            "\n",
            "Epoch 92/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1176 - val_loss: 0.5978\n",
            "\n",
            "Epoch 93/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1203 - val_loss: 0.5978\n",
            "\n",
            "Epoch 94/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1127 - val_loss: 0.5978\n",
            "\n",
            "Epoch 95/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1166 - val_loss: 0.5978\n",
            "\n",
            "Epoch 96/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1108 - val_loss: 0.5978\n",
            "\n",
            "Epoch 97/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1162 - val_loss: 0.5978\n",
            "\n",
            "Epoch 98/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1100 - val_loss: 0.5978\n",
            "\n",
            "Epoch 99/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1083 - val_loss: 0.5978\n",
            "\n",
            "Epoch 100/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1134 - val_loss: 0.5978\n",
            "\n",
            "Epoch 101/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1131 - val_loss: 0.5978\n",
            "\n",
            "Epoch 102/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1122 - val_loss: 0.5978\n",
            "\n",
            "Epoch 103/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1039 - val_loss: 0.5978\n",
            "\n",
            "Epoch 104/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1120 - val_loss: 0.5978\n",
            "\n",
            "Epoch 105/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1085 - val_loss: 0.5978\n",
            "\n",
            "Epoch 106/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1041 - val_loss: 0.5978\n",
            "\n",
            "Epoch 107/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1089 - val_loss: 0.5978\n",
            "\n",
            "Epoch 108/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1100 - val_loss: 0.5978\n",
            "\n",
            "Epoch 109/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1057 - val_loss: 0.5978\n",
            "\n",
            "Epoch 110/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1054 - val_loss: 0.5978\n",
            "\n",
            "Epoch 111/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1101 - val_loss: 0.5978\n",
            "\n",
            "Epoch 112/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1045 - val_loss: 0.5978\n",
            "\n",
            "Epoch 113/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1066 - val_loss: 0.5978\n",
            "\n",
            "Epoch 114/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1026 - val_loss: 0.5978\n",
            "\n",
            "Epoch 115/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0990 - val_loss: 0.5978\n",
            "\n",
            "Epoch 116/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0978 - val_loss: 0.5978\n",
            "\n",
            "45/45 [==============================]\n",
            " - 0s 334us/step\n",
            "\n",
            "Window size 7 score = 0.20483630895614624\n",
            "Window size is 6\n",
            "  9%|▉         | 9/100 [04:54<57:10, 37.69s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1991-01-22</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.285945</td>\n",
              "      <td>0.272833</td>\n",
              "      <td>0.274789</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1991-09-03</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.805993</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.815060</td>\n",
              "      <td>0.857685</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1991-05-21</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1991-07-09</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.295861</td>\n",
              "      <td>0.282524</td>\n",
              "      <td>0.289122</td>\n",
              "      <td>0.266723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1991-03-05</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.261587</td>\n",
              "      <td>0.249026</td>\n",
              "      <td>0.251792</td>\n",
              "      <td>0.226377</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1991-06-18</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.213839</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.227404</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991-01-08</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.274736</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.287838</td>\n",
              "      <td>0.282840</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1991-01-29</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.256090</td>\n",
              "      <td>0.243653</td>\n",
              "      <td>0.249011</td>\n",
              "      <td>0.223639</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1991-07-16</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.294029</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.306985</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "3  1991-01-22  0.249026  0.285945  0.272833  0.274789   0.249026     0.0\n",
              "35 1991-09-03  0.857685  0.805993  0.857685  0.815060   0.857685     0.0\n",
              "20 1991-05-21  0.029917  0.017461  0.029917  0.026527   0.029917     0.0\n",
              "27 1991-07-09  0.266723  0.295861  0.282524  0.289122   0.266723     0.0\n",
              "9  1991-03-05  0.226377  0.261587  0.249026  0.251792   0.226377     0.0\n",
              "24 1991-06-18  0.256505  0.213839  0.256505  0.227404   0.256505     0.0\n",
              "1  1991-01-08  0.282840  0.274736  0.282840  0.287838   0.282840     0.0\n",
              "4  1991-01-29  0.223639  0.256090  0.243653  0.249011   0.223639     0.0\n",
              "19 1991-05-14  0.000000  0.000000  0.000000  0.000000   0.000000     0.0\n",
              "28 1991-07-16  0.318656  0.294029  0.318656  0.306985   0.318656     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "36/36 [==============================]\n",
            " - 14s 379ms/step - loss: 0.2995 - val_loss: 1.0641\n",
            "\n",
            "Epoch 2/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2981 - val_loss: 1.0634\n",
            "\n",
            "Epoch 3/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2966 - val_loss: 1.0627\n",
            "\n",
            "Epoch 4/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2950 - val_loss: 1.0613\n",
            "\n",
            "Epoch 5/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2929 - val_loss: 1.0590\n",
            "\n",
            "Epoch 6/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2909 - val_loss: 1.0550\n",
            "\n",
            "Epoch 7/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2880 - val_loss: 1.0490\n",
            "\n",
            "Epoch 8/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2840 - val_loss: 1.0398\n",
            "\n",
            "Epoch 9/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2789 - val_loss: 1.0262\n",
            "\n",
            "Epoch 10/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2735 - val_loss: 1.0064\n",
            "\n",
            "Epoch 11/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2644 - val_loss: 0.9785\n",
            "\n",
            "Epoch 12/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2552 - val_loss: 0.9402\n",
            "\n",
            "Epoch 13/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2429 - val_loss: 0.8895\n",
            "\n",
            "Epoch 14/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2267 - val_loss: 0.8255\n",
            "\n",
            "Epoch 15/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2101 - val_loss: 0.7495\n",
            "\n",
            "Epoch 16/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1810 - val_loss: 0.6654\n",
            "\n",
            "Epoch 17/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1625 - val_loss: 0.5958\n",
            "\n",
            "Epoch 18/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1397 - val_loss: 0.5530\n",
            "\n",
            "Epoch 19/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1349 - val_loss: 0.5498\n",
            "\n",
            "Epoch 20/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1371 - val_loss: 0.5498\n",
            "\n",
            "Epoch 21/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1352 - val_loss: 0.5498\n",
            "\n",
            "Epoch 22/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1281 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1307 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1148 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1150 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1269 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1232 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1233 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1240 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1178 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1183 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1214 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1172 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1139 - val_loss: 0.5498\n",
            "\n",
            "Epoch 35/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1181 - val_loss: 0.5498\n",
            "\n",
            "Epoch 36/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1212 - val_loss: 0.5498\n",
            "\n",
            "Epoch 37/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1164 - val_loss: 0.5498\n",
            "\n",
            "Epoch 38/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1203 - val_loss: 0.5498\n",
            "\n",
            "Epoch 39/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1185 - val_loss: 0.5498\n",
            "\n",
            "Epoch 40/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1188 - val_loss: 0.5498\n",
            "\n",
            "Epoch 41/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1247 - val_loss: 0.5498\n",
            "\n",
            "Epoch 42/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1155 - val_loss: 0.5498\n",
            "\n",
            "Epoch 43/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1184 - val_loss: 0.5498\n",
            "\n",
            "Epoch 44/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1160 - val_loss: 0.5498\n",
            "\n",
            "Epoch 45/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1148 - val_loss: 0.5498\n",
            "\n",
            "Epoch 46/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1187 - val_loss: 0.5498\n",
            "\n",
            "Epoch 47/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1125 - val_loss: 0.5498\n",
            "\n",
            "Epoch 48/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1155 - val_loss: 0.5498\n",
            "\n",
            "Epoch 49/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1149 - val_loss: 0.5498\n",
            "\n",
            "Epoch 50/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1139 - val_loss: 0.5498\n",
            "\n",
            "Epoch 51/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1107 - val_loss: 0.5730\n",
            "\n",
            "Epoch 52/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1110 - val_loss: 0.6200\n",
            "\n",
            "Epoch 53/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1133 - val_loss: 0.6688\n",
            "\n",
            "Epoch 54/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1029 - val_loss: 0.7238\n",
            "\n",
            "Epoch 55/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1086 - val_loss: 0.7753\n",
            "\n",
            "Epoch 56/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1083 - val_loss: 0.8228\n",
            "\n",
            "Epoch 57/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1046 - val_loss: 0.8486\n",
            "\n",
            "Epoch 58/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1087 - val_loss: 0.8503\n",
            "\n",
            "Epoch 59/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1119 - val_loss: 0.8358\n",
            "\n",
            "Epoch 60/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1076 - val_loss: 0.8144\n",
            "\n",
            "Epoch 61/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0994 - val_loss: 0.7848\n",
            "\n",
            "Epoch 62/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1017 - val_loss: 0.7562\n",
            "\n",
            "Epoch 63/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1021 - val_loss: 0.7266\n",
            "\n",
            "Epoch 64/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0976 - val_loss: 0.7050\n",
            "\n",
            "Epoch 65/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1041 - val_loss: 0.6889\n",
            "\n",
            "Epoch 66/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1017 - val_loss: 0.6796\n",
            "\n",
            "Epoch 67/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1076 - val_loss: 0.6762\n",
            "\n",
            "Epoch 68/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1083 - val_loss: 0.6775\n",
            "\n",
            "Epoch 69/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0985 - val_loss: 0.6862\n",
            "\n",
            "Epoch 70/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0998 - val_loss: 0.6970\n",
            "\n",
            "Epoch 71/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0956 - val_loss: 0.7106\n",
            "\n",
            "Epoch 72/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0950 - val_loss: 0.7240\n",
            "\n",
            "Epoch 73/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1079 - val_loss: 0.7290\n",
            "\n",
            "Epoch 74/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0996 - val_loss: 0.7290\n",
            "\n",
            "Epoch 75/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1058 - val_loss: 0.7239\n",
            "\n",
            "Epoch 76/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1032 - val_loss: 0.7238\n",
            "\n",
            "Epoch 77/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1016 - val_loss: 0.7228\n",
            "\n",
            "Epoch 78/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1015 - val_loss: 0.7226\n",
            "\n",
            "Epoch 79/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1073 - val_loss: 0.7214\n",
            "\n",
            "Epoch 80/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0988 - val_loss: 0.7225\n",
            "\n",
            "Epoch 81/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1017 - val_loss: 0.7260\n",
            "\n",
            "Epoch 82/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1006 - val_loss: 0.7276\n",
            "\n",
            "Epoch 83/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1054 - val_loss: 0.7213\n",
            "\n",
            "Epoch 84/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1061 - val_loss: 0.7106\n",
            "\n",
            "Epoch 85/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0962 - val_loss: 0.7028\n",
            "\n",
            "Epoch 86/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0993 - val_loss: 0.6943\n",
            "\n",
            "Epoch 87/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1056 - val_loss: 0.6852\n",
            "\n",
            "Epoch 88/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1012 - val_loss: 0.6755\n",
            "\n",
            "Epoch 89/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1001 - val_loss: 0.6750\n",
            "\n",
            "Epoch 90/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0994 - val_loss: 0.6788\n",
            "\n",
            "Epoch 91/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1004 - val_loss: 0.6835\n",
            "\n",
            "Epoch 92/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1004 - val_loss: 0.6867\n",
            "\n",
            "Epoch 93/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0975 - val_loss: 0.6866\n",
            "\n",
            "Epoch 94/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0907 - val_loss: 0.6820\n",
            "\n",
            "Epoch 95/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1041 - val_loss: 0.6765\n",
            "\n",
            "Epoch 96/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0997 - val_loss: 0.6730\n",
            "\n",
            "Epoch 97/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0985 - val_loss: 0.6697\n",
            "\n",
            "Epoch 98/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1035 - val_loss: 0.6637\n",
            "\n",
            "Epoch 99/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0982 - val_loss: 0.6621\n",
            "\n",
            "Epoch 100/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0959 - val_loss: 0.6648\n",
            "\n",
            "Epoch 101/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0967 - val_loss: 0.6744\n",
            "\n",
            "Epoch 102/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0905 - val_loss: 0.6830\n",
            "\n",
            "Epoch 103/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.6926\n",
            "\n",
            "Epoch 104/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1050 - val_loss: 0.7010\n",
            "\n",
            "Epoch 105/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0990 - val_loss: 0.7045\n",
            "\n",
            "Epoch 106/1000\n",
            "36/36 [==============================]\n",
            " - 0s 3ms/step - loss: 0.1003 - val_loss: 0.7071\n",
            "\n",
            "Epoch 107/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0929 - val_loss: 0.7067\n",
            "\n",
            "Epoch 108/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0994 - val_loss: 0.6988\n",
            "\n",
            "Epoch 109/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0970 - val_loss: 0.6905\n",
            "\n",
            "Epoch 110/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0998 - val_loss: 0.6824\n",
            "\n",
            "Epoch 111/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0998 - val_loss: 0.6690\n",
            "\n",
            "Epoch 112/1000\n",
            "36/36 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0965 - val_loss: 0.6620\n",
            "\n",
            "Epoch 113/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0888 - val_loss: 0.6561\n",
            "\n",
            "Epoch 114/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0881 - val_loss: 0.6454\n",
            "\n",
            "Epoch 115/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0912 - val_loss: 0.6406\n",
            "\n",
            "Epoch 116/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0906 - val_loss: 0.6383\n",
            "\n",
            "Epoch 117/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0903 - val_loss: 0.6394\n",
            "\n",
            "Epoch 118/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0933 - val_loss: 0.6483\n",
            "\n",
            "Epoch 119/1000\n",
            "36/36 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0914 - val_loss: 0.6586\n",
            "\n",
            "46/46 [==============================]\n",
            " - 0s 251us/step\n",
            "\n",
            "Window size 6 score = 0.21447239816188812\n",
            "Window size is 5\n",
            " 10%|█         | 10/100 [05:43<1:01:34, 41.05s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1991-06-25</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.283466</td>\n",
              "      <td>0.313283</td>\n",
              "      <td>0.296502</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1991-10-08</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.865488</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.874104</td>\n",
              "      <td>0.914885</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1991-06-11</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.169217</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.183121</td>\n",
              "      <td>0.189087</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1991-05-21</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.017461</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.029917</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1991-04-16</td>\n",
              "      <td>0.092805</td>\n",
              "      <td>0.120177</td>\n",
              "      <td>0.110818</td>\n",
              "      <td>0.116162</td>\n",
              "      <td>0.092805</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1991-04-23</td>\n",
              "      <td>0.073001</td>\n",
              "      <td>0.097004</td>\n",
              "      <td>0.088170</td>\n",
              "      <td>0.096053</td>\n",
              "      <td>0.073001</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1991-08-06</td>\n",
              "      <td>0.502686</td>\n",
              "      <td>0.457318</td>\n",
              "      <td>0.502686</td>\n",
              "      <td>0.469034</td>\n",
              "      <td>0.502686</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1991-08-13</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.537616</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.548722</td>\n",
              "      <td>0.583483</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1991-02-26</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.288209</td>\n",
              "      <td>0.275045</td>\n",
              "      <td>0.280458</td>\n",
              "      <td>0.254609</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1991-01-01</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "25 1991-06-25  0.305804  0.283466  0.313283  0.296502   0.305804     0.0\n",
              "40 1991-10-08  0.914885  0.865488  0.914885  0.874104   0.914885     0.0\n",
              "23 1991-06-11  0.189087  0.169217  0.189087  0.183121   0.189087     0.0\n",
              "20 1991-05-21  0.029917  0.017461  0.029917  0.026527   0.029917     0.0\n",
              "15 1991-04-16  0.092805  0.120177  0.110818  0.116162   0.092805     0.0\n",
              "16 1991-04-23  0.073001  0.097004  0.088170  0.096053   0.073001     0.0\n",
              "31 1991-08-06  0.502686  0.457318  0.502686  0.469034   0.502686     0.0\n",
              "32 1991-08-13  0.583483  0.537616  0.583483  0.548722   0.583483     0.0\n",
              "8  1991-02-26  0.254609  0.288209  0.275045  0.280458   0.254609     0.0\n",
              "0  1991-01-01  0.255346  0.233348  0.255346  0.246764   0.255346     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 37 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "37/37 [==============================]\n",
            " - 13s 362ms/step - loss: 0.3202 - val_loss: 1.0141\n",
            "\n",
            "Epoch 2/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3187 - val_loss: 1.0156\n",
            "\n",
            "Epoch 3/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3171 - val_loss: 1.0173\n",
            "\n",
            "Epoch 4/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3155 - val_loss: 1.0189\n",
            "\n",
            "Epoch 5/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3130 - val_loss: 1.0206\n",
            "\n",
            "Epoch 6/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3104 - val_loss: 1.0222\n",
            "\n",
            "Epoch 7/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3075 - val_loss: 1.0233\n",
            "\n",
            "Epoch 8/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.3039 - val_loss: 1.0235\n",
            "\n",
            "Epoch 9/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2986 - val_loss: 1.0222\n",
            "\n",
            "Epoch 10/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2950 - val_loss: 1.0185\n",
            "\n",
            "Epoch 11/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2887 - val_loss: 1.0112\n",
            "\n",
            "Epoch 12/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2799 - val_loss: 0.9993\n",
            "\n",
            "Epoch 13/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2688 - val_loss: 0.9811\n",
            "\n",
            "Epoch 14/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2579 - val_loss: 0.9545\n",
            "\n",
            "Epoch 15/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.2415 - val_loss: 0.9180\n",
            "\n",
            "Epoch 16/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.2237 - val_loss: 0.8701\n",
            "\n",
            "Epoch 17/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1999 - val_loss: 0.8099\n",
            "\n",
            "Epoch 18/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1755 - val_loss: 0.7389\n",
            "\n",
            "Epoch 19/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1546 - val_loss: 0.6643\n",
            "\n",
            "Epoch 20/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1455 - val_loss: 0.6058\n",
            "\n",
            "Epoch 21/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1280 - val_loss: 0.5650\n",
            "\n",
            "Epoch 22/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1147 - val_loss: 0.5498\n",
            "\n",
            "Epoch 23/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1100 - val_loss: 0.5498\n",
            "\n",
            "Epoch 24/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1122 - val_loss: 0.5498\n",
            "\n",
            "Epoch 25/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1137 - val_loss: 0.5498\n",
            "\n",
            "Epoch 26/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.1255 - val_loss: 0.5498\n",
            "\n",
            "Epoch 27/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1225 - val_loss: 0.5498\n",
            "\n",
            "Epoch 28/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1064 - val_loss: 0.5498\n",
            "\n",
            "Epoch 29/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1148 - val_loss: 0.5498\n",
            "\n",
            "Epoch 30/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1270 - val_loss: 0.5498\n",
            "\n",
            "Epoch 31/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1363 - val_loss: 0.5498\n",
            "\n",
            "Epoch 32/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1201 - val_loss: 0.5498\n",
            "\n",
            "Epoch 33/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1077 - val_loss: 0.5498\n",
            "\n",
            "Epoch 34/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1139 - val_loss: 0.5498\n",
            "\n",
            "Epoch 35/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1251 - val_loss: 0.5498\n",
            "\n",
            "Epoch 36/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1261 - val_loss: 0.5498\n",
            "\n",
            "Epoch 37/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1077 - val_loss: 0.5498\n",
            "\n",
            "Epoch 38/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1090 - val_loss: 0.5498\n",
            "\n",
            "Epoch 39/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1108 - val_loss: 0.5498\n",
            "\n",
            "Epoch 40/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1115 - val_loss: 0.5498\n",
            "\n",
            "Epoch 41/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1079 - val_loss: 0.5498\n",
            "\n",
            "Epoch 42/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1054 - val_loss: 0.5498\n",
            "\n",
            "Epoch 43/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1084 - val_loss: 0.5498\n",
            "\n",
            "Epoch 44/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1047 - val_loss: 0.5498\n",
            "\n",
            "Epoch 45/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1141 - val_loss: 0.5498\n",
            "\n",
            "Epoch 46/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.5498\n",
            "\n",
            "Epoch 47/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1077 - val_loss: 0.5498\n",
            "\n",
            "Epoch 48/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1112 - val_loss: 0.5498\n",
            "\n",
            "Epoch 49/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1119 - val_loss: 0.5498\n",
            "\n",
            "Epoch 50/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1076 - val_loss: 0.5498\n",
            "\n",
            "Epoch 51/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1064 - val_loss: 0.5498\n",
            "\n",
            "Epoch 52/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1074 - val_loss: 0.5498\n",
            "\n",
            "Epoch 53/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1063 - val_loss: 0.5498\n",
            "\n",
            "Epoch 54/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1024 - val_loss: 0.5498\n",
            "\n",
            "Epoch 55/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1087 - val_loss: 0.5589\n",
            "\n",
            "Epoch 56/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1026 - val_loss: 0.5881\n",
            "\n",
            "Epoch 57/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1014 - val_loss: 0.6201\n",
            "\n",
            "Epoch 58/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0983 - val_loss: 0.6552\n",
            "\n",
            "Epoch 59/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1027 - val_loss: 0.6897\n",
            "\n",
            "Epoch 60/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1000 - val_loss: 0.7292\n",
            "\n",
            "Epoch 61/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0908 - val_loss: 0.7653\n",
            "\n",
            "Epoch 62/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0939 - val_loss: 0.7962\n",
            "\n",
            "Epoch 63/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0982 - val_loss: 0.8172\n",
            "\n",
            "Epoch 64/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0964 - val_loss: 0.8231\n",
            "\n",
            "Epoch 65/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1038 - val_loss: 0.8175\n",
            "\n",
            "Epoch 66/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0969 - val_loss: 0.8061\n",
            "\n",
            "Epoch 67/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0974 - val_loss: 0.7900\n",
            "\n",
            "Epoch 68/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0975 - val_loss: 0.7683\n",
            "\n",
            "Epoch 69/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0949 - val_loss: 0.7495\n",
            "\n",
            "Epoch 70/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0937 - val_loss: 0.7344\n",
            "\n",
            "Epoch 71/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0953 - val_loss: 0.7244\n",
            "\n",
            "Epoch 72/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0919 - val_loss: 0.7199\n",
            "\n",
            "Epoch 73/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0962 - val_loss: 0.7202\n",
            "\n",
            "Epoch 74/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1002 - val_loss: 0.7235\n",
            "\n",
            "Epoch 75/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0960 - val_loss: 0.7287\n",
            "\n",
            "Epoch 76/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0971 - val_loss: 0.7346\n",
            "\n",
            "Epoch 77/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0929 - val_loss: 0.7417\n",
            "\n",
            "Epoch 78/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.1005 - val_loss: 0.7498\n",
            "\n",
            "Epoch 79/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0895 - val_loss: 0.7514\n",
            "\n",
            "Epoch 80/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0911 - val_loss: 0.7549\n",
            "\n",
            "Epoch 81/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0912 - val_loss: 0.7544\n",
            "\n",
            "Epoch 82/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0899 - val_loss: 0.7548\n",
            "\n",
            "Epoch 83/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0940 - val_loss: 0.7577\n",
            "\n",
            "Epoch 84/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0895 - val_loss: 0.7565\n",
            "\n",
            "Epoch 85/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0966 - val_loss: 0.7565\n",
            "\n",
            "Epoch 86/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0910 - val_loss: 0.7492\n",
            "\n",
            "Epoch 87/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0879 - val_loss: 0.7431\n",
            "\n",
            "Epoch 88/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0904 - val_loss: 0.7393\n",
            "\n",
            "Epoch 89/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0913 - val_loss: 0.7325\n",
            "\n",
            "Epoch 90/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0909 - val_loss: 0.7269\n",
            "\n",
            "Epoch 91/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0933 - val_loss: 0.7235\n",
            "\n",
            "Epoch 92/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0919 - val_loss: 0.7236\n",
            "\n",
            "Epoch 93/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0939 - val_loss: 0.7238\n",
            "\n",
            "Epoch 94/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0944 - val_loss: 0.7254\n",
            "\n",
            "Epoch 95/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0935 - val_loss: 0.7281\n",
            "\n",
            "Epoch 96/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0980 - val_loss: 0.7336\n",
            "\n",
            "Epoch 97/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0888 - val_loss: 0.7378\n",
            "\n",
            "Epoch 98/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0903 - val_loss: 0.7432\n",
            "\n",
            "Epoch 99/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0949 - val_loss: 0.7515\n",
            "\n",
            "Epoch 100/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0926 - val_loss: 0.7607\n",
            "\n",
            "Epoch 101/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0894 - val_loss: 0.7636\n",
            "\n",
            "Epoch 102/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0926 - val_loss: 0.7603\n",
            "\n",
            "Epoch 103/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0947 - val_loss: 0.7589\n",
            "\n",
            "Epoch 104/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0914 - val_loss: 0.7509\n",
            "\n",
            "Epoch 105/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0916 - val_loss: 0.7374\n",
            "\n",
            "Epoch 106/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0903 - val_loss: 0.7294\n",
            "\n",
            "Epoch 107/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0904 - val_loss: 0.7235\n",
            "\n",
            "Epoch 108/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0909 - val_loss: 0.7174\n",
            "\n",
            "Epoch 109/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0950 - val_loss: 0.7140\n",
            "\n",
            "Epoch 110/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0885 - val_loss: 0.7102\n",
            "\n",
            "Epoch 111/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0894 - val_loss: 0.7063\n",
            "\n",
            "Epoch 112/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0967 - val_loss: 0.7054\n",
            "\n",
            "Epoch 113/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0881 - val_loss: 0.7067\n",
            "\n",
            "Epoch 114/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0904 - val_loss: 0.7126\n",
            "\n",
            "Epoch 115/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0900 - val_loss: 0.7181\n",
            "\n",
            "Epoch 116/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0894 - val_loss: 0.7229\n",
            "\n",
            "Epoch 117/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0889 - val_loss: 0.7265\n",
            "\n",
            "Epoch 118/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0890 - val_loss: 0.7293\n",
            "\n",
            "Epoch 119/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0921 - val_loss: 0.7251\n",
            "\n",
            "Epoch 120/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0893 - val_loss: 0.7209\n",
            "\n",
            "Epoch 121/1000\n",
            "37/37 [==============================]\n",
            " - 0s 1ms/step - loss: 0.0892 - val_loss: 0.7126\n",
            "\n",
            "Epoch 122/1000\n",
            "37/37 [==============================]\n",
            " - 0s 2ms/step - loss: 0.0880 - val_loss: 0.7070\n",
            "\n",
            "47/47 [==============================]\n",
            " - 0s 219us/step\n",
            "\n",
            "Window size 5 score = 0.21740737557411194\n",
            "Window size is 3\n",
            " 11%|█         | 11/100 [06:31<1:03:48, 43.01s/it, best loss: 0.1557055115699768]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1991-01-01</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.233348</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.246764</td>\n",
              "      <td>0.255346</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1991-07-23</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.347058</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.359611</td>\n",
              "      <td>0.354998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1991-12-31</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1991-07-16</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.294029</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.306985</td>\n",
              "      <td>0.318656</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1991-05-14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1991-02-12</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.273612</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1991-05-07</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.039125</td>\n",
              "      <td>0.031602</td>\n",
              "      <td>0.022141</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1991-03-12</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.232593</td>\n",
              "      <td>0.220689</td>\n",
              "      <td>0.202589</td>\n",
              "      <td>0.177921</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1991-01-15</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.296939</td>\n",
              "      <td>0.283577</td>\n",
              "      <td>0.304525</td>\n",
              "      <td>0.278310</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1991-06-18</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.213839</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.227404</td>\n",
              "      <td>0.256505</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date     Close      Open      High       Low  Adj Close  Volume\n",
              "0  1991-01-01  0.255346  0.233348  0.255346  0.246764   0.255346     0.0\n",
              "29 1991-07-23  0.354998  0.347058  0.354998  0.359611   0.354998     0.0\n",
              "52 1991-12-31  2.000000  2.000000  2.000000  2.000000   2.000000     0.0\n",
              "28 1991-07-16  0.318656  0.294029  0.318656  0.306985   0.318656     0.0\n",
              "19 1991-05-14  0.000000  0.000000  0.000000  0.000000   0.000000     0.0\n",
              "6  1991-02-12  0.260297  0.260401  0.260297  0.273612   0.260297     0.0\n",
              "18 1991-05-07  0.000211  0.039125  0.031602  0.022141   0.000211     0.0\n",
              "10 1991-03-12  0.177921  0.232593  0.220689  0.202589   0.177921     0.0\n",
              "2  1991-01-15  0.278310  0.296939  0.283577  0.304525   0.278310     0.0\n",
              "24 1991-06-18  0.256505  0.213839  0.256505  0.227404   0.256505     0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 39 samples, validate on 10 samples\n",
            "Epoch 1/1000\n",
            "39/39 [==============================]\n",
            " - 16s 399ms/step - loss: 0.3511 - val_loss: 0.9504\n",
            "\n",
            " 11%|█         | 11/100 [06:51<1:03:48, 43.01s/it, best loss: 0.1557055115699768]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-47b6f715cff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     bayes_best = fmin(fn=fmin_objective, space=param_grid,\n\u001b[1;32m     96\u001b[0m                       \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbayes_algo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbayes_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                       max_evals=bayer_max_evals)\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mof_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayer_save_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-47b6f715cff2>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params, df)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Perform n_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-34a264060622>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_valid, y_valid, stock_name, year, window_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         shuffle=False)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    222\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    717\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_write_to_gcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_wrapper\u001b[0;34m(obj, filepath, overwrite, *args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msave_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# write as binary stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, h5dict, include_optimizer)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msymbolic_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0moptimizer_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 for i, (w, val) in enumerate(zip(symbolic_weights,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     \"\"\"\n\u001b[1;32m   2681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}