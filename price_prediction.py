# -*- coding: utf-8 -*-
"""working.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1llsccnklCzs7EZSELP6MVysncixaZnQR

## Notebook settings
"""
# region Import
# Data download
# Import basic
import csv
import math
import os
import warnings
# Init google drive
# from google.colab import drive
from datetime import datetime
from timeit import default_timer as timer

import numpy as np
import pandas as pd
# Plottool
import plotly.graph_objs as go
# IPython
from IPython.display import display
# Hyperopt bayesian optimization
from hyperopt import hp, Trials, tpe, fmin, STATUS_OK, partial
# Keras
from keras import Sequential
from keras.activations import softmax
from keras.callbacks import EarlyStopping
from keras.initializers import Ones
from keras.layers import LSTM, Dropout, Input
from keras.models import Model
# SKLearn
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# endregion

# region File mount and config
# drive.mount('/content/gdrive', force_remount=True)
root_dir = ""
data_dir = root_dir + 'stock'
model_dir = root_dir + 'model'

pd.options.display.max_columns = 12
pd.options.display.max_rows = 24

# disable warnings in Anaconda
warnings.filterwarnings('ignore')

# endregion

# region Data Loading
stock_name = '000001.SS'  # SSE Composite Index
# df_org = yf.download(stock_name, start="1991-01-01", end="2016-12-31", interval="1wk")
# df_org.to_csv(f'{base_dir}/{stock_name}.csv')
df_org = pd.read_csv(f'{data_dir}/{stock_name}.csv', parse_dates=['Date'])
df_org = df_org.sort_values('Date')
df_org.reset_index(inplace=True)
df_org = df_org[['Date', 'Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']]


# endregion

# region Data ploting
def plot_ohlc(df):
    trace = go.Ohlc(x=df['Date'],
                    open=df['Open'],
                    high=df['High'],
                    low=df['Low'],
                    close=df['Close'],
                    increasing=dict(line=dict(color='#58FA58')),
                    decreasing=dict(line=dict(color='#FA5858')))

    layout = {
        'title': f'{stock_name} Historical Price',
        'xaxis': {'title': 'Date',
                  'rangeslider': {'visible': False}},
        'yaxis': {'title': f'Price'}
    }

    data = [trace]

    fig = go.Figure(data=data, layout=layout)
    fig.write_html(f'ohlc_{stock_name}.html', auto_open=True)
    # fig.show()


plot_ohlc(df_org)
# endregion

# region Create csv result file
# File to save first results
save_fname = os.path.join(data_dir, '%s-%s.csv' % (stock_name, datetime.now().strftime('%d%m%Y-%H%M%S')))
of_connection = open(save_fname, 'w')
writer = csv.writer(of_connection)
# Write the headers to the file
writer.writerow(['stock_name', 'year', 'loss', 'params', 'iteration', 'windows_size', 'train_time'])
of_connection.close()
# endregion

# region Sample data

# df_org = df_org[:365]
df_org.head(10)

# endregion

# region Const
# Declare const
input_col = ['Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']
output_col = ['Close']
time_col = ['Date']

# Input dimension
input_dim = len(input_col)
# Output dimension
output_dim = len(output_col)

# Number of session to prediction as one time
prediction_size = 1
# For each time model is train, the first is display
sample_display_test_size = 5
# Max bayer iteration
bayer_max_evals = 1


# endregion

# region Declare model
# declare model
def softMaxAxis1(x):
    return softmax(x, axis=1)


def get_model(input_dim, window_size, output_dim, lstm_layer_count=5, drop_rate=0.2):
    # x = Input(shape=(window_size, input_dim))
    # lstm = LSTM(units=100, return_sequences=True, kernel_initializer=Ones())(x)
    # lstm = Dropout(rate=drop_rate)(lstm)
    #
    # for i in range(lstm_layer_count - 1):
    #     lstm = LSTM(units=100, return_sequences=True, kernel_initializer=Ones())(lstm)
    #     lstm = Dropout(rate=drop_rate)(lstm)
    #
    # predictions = LSTM(units=output_dim, activation=softMaxAxis1)(lstm)
    #
    # model = Model(inputs=x, outputs=predictions)
    # model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])
    # model.summary()

    model = Sequential()
    model.add(LSTM(units=100, input_shape=(window_size, input_dim), return_sequences=True, kernel_initializer=Ones()))
    model.add(Dropout(rate=0.2))
    model.add(LSTM(units=100, return_sequences=True))
    model.add(Dropout(rate=0.2))
    model.add(LSTM(units=100, return_sequences=True))
    model.add(Dropout(rate=0.2))
    model.add(LSTM(units=100, return_sequences=True))
    model.add(Dropout(rate=0.2))
    model.add(LSTM(output_dim, activation=softMaxAxis1))
    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])


    return model


# endregion

# region Error metric
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)

    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


def root_mean_square_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)

    return np.mean((y_true - y_pred) / y_true)


def relative_root_mean_square_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    res = (y_true - y_pred) / y_true
    res = np.power(res, 2)
    res = np.mean(res)
    res = math.sqrt(res)

    return res


# endregion

# region Data preprocessing
# reprocessing data
def next_window(df, i, windows_size, prediction_size, input_col, output_col, time_col):
    '''Generates the next data window from the given index location i'''
    window = df[i: i + windows_size + prediction_size]
    x = window[input_col][:-prediction_size]
    y = window[output_col][-prediction_size:]
    y_time = window[time_col][-prediction_size:]
    return x, y, y_time

def smooting_data(df, window_size):
    return df.ewm(span=window_size).mean()

def preprocessing_data(df, windows_size, prediction_size, input_col, output_col, time_col):
    '''
    Create x, y train data windows
    Warning: batch method, not generative, make sure you have enough memory to
    load data, otherwise use generate_training_window() method.
    '''


    data_x = []
    data_y = []
    data_y_time = []
    for i in range(len(df) - windows_size - prediction_size):
        x, y, y_time = next_window(df, i, windows_size, prediction_size, input_col, output_col, time_col)
        data_x.append(x.values)
        data_y.append(y.values)
        data_y_time.append(y_time)

    time = pd.concat(data_y_time)

    return np.array(data_x), np.array(data_y), time.values


def split_train_test_data(X, y):
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)

    return X_train, y_train, X_valid, y_valid


# endregion

# region Model train
# Trainning model
def train_model(model, X_train, y_train, X_valid, y_valid):
    callbacks = [
        EarlyStopping(monitor='loss', patience=100)
    ]
    history = model.fit(
        X_train,
        y_train,
        epochs=1000,
        batch_size=10000,
        validation_data=(X_valid, y_valid),
        verbose=1,
        callbacks=callbacks,
        shuffle=False)

    return history


# endregion

# region Test model
def test_model(model, test_data, window_size, prediction_size, input_col, output_col, time_col):
    X, y, time = preprocessing_data(test_data, window_size, prediction_size, input_col, output_col, time_col)
    y = y.reshape((y.shape[0], y.shape[1]))

    y_pred = model.predict(X)

    y_pred = np.repeat(y_pred, input_dim, axis=1)
    y_pred = scaler.inverse_transform(y_pred)[:, [0]]
    y_pred = pd.Series(y_pred.flatten())

    df_test_result = pd.DataFrame(time, columns=['Date'])
    df_test_result['Prediction'] = y_pred
    df_test_result.set_index('Date', inplace=True)

    return df_test_result


def plot_test_result(test_result):
    # Plotly
    trace0 = go.Scatter(
        x=test_result.index,
        y=test_result['Close'],
        name='Thực tế',
        line=dict(
            color=('#5042f4'),
            width=2)
    )

    trace1 = go.Scatter(
        x=test_result.index,
        y=test_result['Prediction'],
        name='Dự đoán',
        line=dict(
            color=('#005b4e'),
            width=2,
            dash='dot'
        )  # dash options include 'dash', 'dot', and 'dashdot'
    )

    data = [trace0, trace1]

    # Edit the layout
    layout = dict(title='Biểu đồ dự đoán',
                  xaxis=dict(title='Date'),
                  yaxis=dict(title='Price'),
                  paper_bgcolor='#FFF9F5',
                  plot_bgcolor='#FFF9F5'
                  )

    fig = go.Figure(data=data, layout=layout)
    fig.show()


# endregion


# region bayers
def objective(params, df):
    # Keep track of evals
    global ITERATION

    ITERATION += 1

    # Make sure windows_size is int
    windows_size = int(params['windows_size'])
    print(f'Window size is {windows_size}')

    model = get_model(input_dim, windows_size, output_dim)

    start = timer()

    # Handle data
    df.describe()
    # TODO: smoothing ddata
    df = smooting_data(df, windows_size)
    X, y, time = preprocessing_data(df, windows_size, prediction_size, input_col, output_col, time_col)

    # Reshape data
    y = y.reshape((y.shape[0], y.shape[1]))

    X_train, y_train, X_valid, y_valid = split_train_test_data(X, y)

    # Perform n_train
    history = train_model(model, X_train, y_train, X_valid, y_valid)

    run_time = timer() - start

    # Test generated loss
    test_result = test_model(model, df, windows_size, prediction_size, input_col, output_col, time_col)
    test_result = test_result.join(df_org.set_index('Date'))

    mae = mean_absolute_error(test_result['Close'], test_result['Prediction'])
    mse = mean_squared_error(test_result['Close'], test_result['Prediction'])
    mape = mean_absolute_percentage_error(test_result['Close'], test_result['Prediction'])
    rrmse = relative_root_mean_square_error(test_result['Close'], test_result['Prediction'])

    print(f'{stock_name} prediction for {prediction_size} day ahead')
    print(f'MAE = {mae}')
    print(f'MSE = {mse}')
    print(f'MAPE = {mape}')
    print(f'RRMSE = {rrmse}')

    plot_test_result(test_result)
    loss = mae

    # write row
    of_connection = open(save_fname, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([stock_name, year, loss, params, ITERATION, windows_size, run_time])

    # Dictionary with information for evaluation` v
    return {'loss': loss, 'params': params, 'iteration': ITERATION, 'test_result': test_result,
            'train_time': run_time, 'status': STATUS_OK}


# Run evals with the tpe algorithm
# bayes_best = fmin(fn=objective, space=param_grid,
#                algo=bayes_algo, trials=bayes_trials, 
#                max_evals=10)
#
# print(bayes_best)

start_year = df_org['Date'].values[:1][0]
start_year = pd.to_datetime(start_year).year

end_year = df_org['Date'].values[-1:][0]
end_year = pd.to_datetime(end_year).year

windows_size_best = []
# Global variable
global ITERATION

for year in range(start_year, end_year + 1):
    df = df_org[df_org['Date'].dt.year == year]

    # Data too small, skip
    if df.shape[0] < 10:
        continue

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_cols = scaler.fit_transform(df[input_col])
    df[input_col] = scaled_cols

    # Hyperparameter grid
    param_grid = {
        'windows_size': hp.choice('windows_size', np.arange(1, 8, dtype=int))
    }

    bayes_trials = Trials()

    # Create the algorithm
    bayes_algo = tpe.suggest

    ITERATION = 0

    fmin_objective = partial(objective, df=df)
    bayes_best = fmin(fn=fmin_objective, space=param_grid,
                      algo=bayes_algo, trials=bayes_trials,
                      max_evals=bayer_max_evals)

    windows_size_best.append([year, bayes_best])
    print(f'LOG:RESULT Bayer best for {year} = {bayes_best}')

print(windows_size_best)

# plot_test_result(bayes_trials.results[-1]['test_result'])

# engregion
